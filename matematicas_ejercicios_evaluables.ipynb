{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObpOWDyPhIeBKt6ohYqdEX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MareMaltese/matematicasAI/blob/main/matematicas_ejercicios_evaluables.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Ejercicios evaluables\n",
        "\n",
        "## Tal y como ya hemos visto en clase, la variedad de herramientas proporcionadas por el algebra lineal son cruciales para desarrollar y fundamentar las bases de una variedad de tecnicas relacionadas con el aprendizaje automático. Con ella, podemos describir el proceso de propagacion hacia adelante en una red neuronal, identificar mínimos locales en funciones multivariables (crucial para el proceso de retropropagación) o la descripción y empleo de metodos de reducción de la dimensionalidad, como el análisis de componentes principales(PCA), entre muchas otras aplicaciones.\n",
        "   \n",
        "Cuando trabajamos en la práctica dentro de este ámbito, la cantidad de datos que manejamos puede ser muy grande, por lo que es especialmente importante emplear algoritmos eficientes y optimizados para reducir el coste computacional en la medida de lo posible. Por todo ello, el objetivo de este ejercicio es el de ilustrar las diferentes alternativas que pueden existir para realizar un proceso relacionado con el álgebra lineal y el impacto que puede tener cada variante en términos del coste computacional del mismo. En este caso en particular, y a modo de ilustración, nos centraremos en el cálculo del determinante de una matriz."
      ],
      "metadata": {
        "id": "gRhjbmfnKPDe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a. [1 punto] Implementa una función, determinante recursivo, que obtenga el determinante de una matriz cuadrada utilizando la definición recursiva de Laplace."
      ],
      "metadata": {
        "id": "rm1QG4V0KRet"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3o7wu9hJoAl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def determinante_recursivo_pandas(matrix_df):\n",
        "    \"\"\"\n",
        "    Calcula el determinante de una matriz cuadrada utilizando la definición recursiva de Laplace.\n",
        "    Usa pandas para optimizar la manipulación de submatrices.\n",
        "    Args:\n",
        "        matrix_df (pd.DataFrame): Matriz cuadrada como DataFrame\n",
        "    Returns:\n",
        "        float: Determinante de la matriz\n",
        "    \"\"\"\n",
        "    # Verificar si la matriz es cuadrada\n",
        "    if matrix_df.shape[0] != matrix_df.shape[1]:\n",
        "        raise ValueError(\"La matriz debe ser cuadrada\")\n",
        "\n",
        "    n = matrix_df.shape[0]\n",
        "\n",
        "    # Caso base: matriz 1x1\n",
        "    if n == 1:\n",
        "        return matrix_df.iloc[0, 0]\n",
        "\n",
        "    # Caso base: matriz 2x2\n",
        "    if n == 2:\n",
        "        return matrix_df.iloc[0, 0] * matrix_df.iloc[1, 1] - matrix_df.iloc[0, 1] * matrix_df.iloc[1, 0]\n",
        "\n",
        "    # Caso general: desarrollo por la primera fila (Laplace)\n",
        "    det = 0\n",
        "    for col in range(n):\n",
        "        # Crear submatriz eliminando la primera fila y la columna actual\n",
        "        submatrix = matrix_df.drop(index=matrix_df.index[0], columns=matrix_df.columns[col])\n",
        "\n",
        "        # Sumar al determinante con el signo alternante\n",
        "        det += ((-1) ** col) * matrix_df.iloc[0, col] * determinante_recursivo_pandas(submatrix)\n",
        "\n",
        "    return det\n",
        "\n",
        "# Ejemplo de uso\n",
        "data = [\n",
        "    [2, 1, 3],\n",
        "    [1, 0, 2],\n",
        "    [4, 1, 1]\n",
        "]\n",
        "\n",
        "# Crear un DataFrame de Pandas\n",
        "matrix_df = pd.DataFrame(data)\n",
        "print(\"Matriz:\")\n",
        "print(matrix_df)\n",
        "\n",
        "# Calcular el determinante\n",
        "det = determinante_recursivo_pandas(matrix_df)\n",
        "print(f\"Determinante (definición recursiva de Laplace usando Pandas): {det}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b. [0.5 puntos] Si A es una matriz cuadrada n×n y triangular (superior o inferior, es decir, con entradas nulas por debajo o por encima de la diagonal, respectivamente), ¿existe alguna forma de calcular de forma directa y sencilla su determinante? Justifíquese la respuesta"
      ],
      "metadata": {
        "id": "glVfVQERLasW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si. Para todas las matrices cuadradas triangulares (es decir, todos los números por encima o por debajo de la diagonal son ceros), su determinante se puede calcular sencillamente multiplicando los números de la diagonal principal.\n",
        "\n",
        "Debido a la **Propiedad Fundamental**:\n",
        "\n",
        "$$\n",
        "A = \\begin{bmatrix}\n",
        "a_{11} & a_{12} & a_{13} \\\\\n",
        "a_{21} & a_{22} & a_{23} \\\\\n",
        "a_{31} & a_{32} & a_{33}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\det(A) = a_{11} \\cdot a_{22} \\cdots a_{nn} = \\prod_{i=1}^n a_{ii}\n",
        "$$\n",
        "\n",
        "\n",
        "Si A es una matriz cuadrada n*n y triangular (superior o inferior), su determinante es igual al producto de los elementos en su diagonal principal.\n",
        "\n",
        "Por ejemplo, si la matriz es:\n",
        "$$\n",
        "A = \\begin{bmatrix}\n",
        "2 & 23 & 13 \\\\\n",
        "0 & 4 & 5 \\\\\n",
        "0 & 0 & 6\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Su determinante será simplemente:\n",
        "2 ⋅ 4 ⋅ 6 = 48"
      ],
      "metadata": {
        "id": "uUalkA5bL2x6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### c. [0.5 puntos] Determínese de forma justificada cómo alteran el determinante de una matriz n × n las dos operaciones elementales siguientes:"
      ],
      "metadata": {
        "id": "RD-RwtXXKfCX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ### Intercambiar una fila (o columna) por otra fila (o columna).\n",
        "\n",
        "Si intercambiamos **dos filas** (o **dos columnas**) de una matriz cuadrada $( n \\times n $), el **determinante cambia de signo**.\n",
        "\n",
        "**Regla general:**\n",
        "Si $( A $) es la matriz original y $( B $) es la matriz después de intercambiar dos filas o columnas, entonces:\n",
        "\n",
        "$$\n",
        "\\det(B) = -\\det(A)\n",
        "$$\n",
        "\n",
        "El determinante de una matriz está relacionado con el **volumen orientado** definido por las filas o columnas de la matriz. **Intercambiar filas o columnas** no cambia el tamaño del volumen, pero sí su **orientación** que se refleja como un **cambio de signo** en el determinante.\n",
        "\n",
        "---\n",
        "\n",
        "**Ejemplo:**\n",
        "\n",
        "Supongamos que tenemos la matriz:\n",
        "\n",
        "$$ A =\n",
        "\\begin{bmatrix}\n",
        "1 & 2 \\\\\n",
        "3 & 4\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "1 - Calculamos su determinante:\n",
        "$$ \\det(A) = 1 \\cdot 4 - 2 \\cdot 3 = -2 $$\n",
        "\n",
        "2 - Si intercambiamos las dos filas, obtenemos la matriz:\n",
        "$$\n",
        "B = \\begin{bmatrix}\n",
        "3 & 4 \\\\\n",
        "1 & 2\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Ahora el determinante de (B) es:\n",
        "$$\n",
        "\\det(B) = 3 \\cdot 2 - 4 \\cdot 1 = 2\n",
        "$$\n",
        "\n",
        "Observamos que:\n",
        "$$\n",
        "\\det(B) = -\\det(A)\n",
        "$$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "b30r_38QXaT2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- ### Sumar a una fila (o columna) otra fila (o columna) multiplicada por un escalar α\n",
        "\n",
        "Si sumamos a una fila (o columna) de una matriz otra fila (o columna) multiplicada por un **escalar** $\\alpha$ , el **determinante no cambia**.\n",
        "\n",
        "#### **Regla general:**\n",
        "Esta operación **no altera el determinante** de la matriz. Es decir, si partimos de una matriz (A) y realizamos esta operación, el determinante sigue siendo el mismo:\n",
        "$$\\det(B) = \\det(A)$$\n",
        "\n",
        "donde (B) es la matriz después de aplicar la operación.\n",
        "\n",
        "\n",
        "El determinante mide el **volumen orientado** definido por las filas o columnas de la matriz.  \n",
        "Cuando sumamos a una fila otra fila multiplicada por un escalar, la fila original se \"desplaza\" en el espacio, sin cambiar el volumen.\n",
        "\n",
        "---\n",
        "\n",
        "**Ejemplo:**\n",
        "\n",
        "Supongamos que tenemos la matriz:\n",
        "$$\n",
        "A =\n",
        "\\begin{bmatrix}\n",
        "1 & 2 \\\\\n",
        "3 & 4\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "1 - Calculamos su determinante:\n",
        "$$\n",
        "\\det(A) = 1 \\cdot 4 - 2 \\cdot 3 = -2\n",
        "$$\n",
        "\n",
        "2 - Ahora sumamos a la **primera fila** la **segunda fila multiplicada por ( $\\alpha$ = 2 )**:\n",
        "$$\n",
        "B =\n",
        "\\begin{bmatrix}\n",
        "1 + 2 \\cdot 3 & 2 + 2 \\cdot 4 \\\\\n",
        "3 & 4\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "7 & 10 \\\\\n",
        "3 & 4\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "3 - Calculamos el determinante de (B):\n",
        "$$\n",
        "\\det(B) = 7 \\cdot 4 - 10 \\cdot 3 = -2\n",
        "$$\n",
        "\n",
        "Comprobado:\n",
        "$$\n",
        "\\det(B) = \\det(A)\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "4xanjyrJZmYX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "d ) [1 punto] Investiga sobre el método de eliminación de Gauss con pivoteo parcial e\n",
        "impleméntalo para escalonar una matriz (es decir, convertirla en una matriz triangular\n",
        "inferior) a partir de las operaciones elementales descritas en el apartado anterior."
      ],
      "metadata": {
        "id": "q2dwWbFVSVwg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Método de Eliminación de Gauss con Pivoteo Parcial**\n",
        "\n",
        "La **eliminación de Gauss con pivoteo parcial** es una técnica utilizada para convertir una matriz $( A $) en una **matriz triangular inferior** (o escalonada) mediante **operaciones elementales**. Es una mejora del método de eliminación de Gauss básico, ya que mejora la **estabilidad numérica**.\n",
        "\n",
        "---\n",
        "\n",
        "### **¿Qué es el pivoteo parcial?**\n",
        "\n",
        "El **pivoteo parcial** consiste en seleccionar, en cada paso, el elemento de mayor valor absoluto de la columna actual (el pivote) y **reorganizar las filas** de la matriz para que dicho elemento quede en la posición diagonal actual.\n",
        "\n",
        "### **Operaciones Elementales Utilizadas**\n",
        "\n",
        "Durante el proceso, aplicamos las siguientes operaciones elementales:\n",
        "\n",
        "1. **Intercambiar filas**: Para asegurar que el pivote sea el mayor valor absoluto.\n",
        "   $[\n",
        "   F_i \\leftrightarrow F_j\n",
        "   ]$\n",
        "\n",
        "2. **Multiplicar una fila por un escalar**: Si una fila $( F_i $) se multiplica por un escalar $( $\\alpha$ $), se tiene:\n",
        "   $[\n",
        "   F_i \\to \\alpha F_i\n",
        "   $]\n",
        "\n",
        "3. **Sumar a una fila otra fila multiplicada por un escalar**: Para eliminar los elementos debajo del pivote:\n",
        "   $[\n",
        "   F_j \\to F_j - \\alpha F_i\n",
        "   $]\n",
        "   donde $( $\\alpha$ $) es el cociente entre el elemento de la fila actual y el pivote.\n",
        "\n",
        "---\n",
        "\n",
        "### **Pasos del Método**\n",
        "\n",
        "1. Para cada columna $( k $) de la matriz:\n",
        "   - **Buscar el pivote**: Encontrar el elemento de mayor valor absoluto en la columna $( k $) a partir de la fila $( k $).\n",
        "   - **Intercambiar filas** si el pivote no está en la posición diagonal actual.\n",
        "2. Usar operaciones elementales para **eliminar los elementos debajo del pivote** y formar ceros en la columna $( k $).\n",
        "3. Repetir este proceso para todas las columnas.\n",
        "\n",
        "Al final del proceso, la matriz quedará **triangular inferior**.\n",
        "\n",
        "---\n",
        " **Ejemplo**\n",
        "\n",
        "Supongamos que tenemos la matriz $( A $):\n",
        "\n",
        "$$\n",
        "A =\n",
        "\\begin{bmatrix}\n",
        "2 & -1 & 1 \\\\\n",
        "-1 & 3 & 2 \\\\\n",
        "1 & -2 & 4\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "**Paso 1: Selección del pivote en la primera columna:**\n",
        "- El mayor valor absoluto en la primera columna es $( 2 $), así que no es necesario intercambiar filas.\n",
        "\n",
        "**Paso 2: Eliminación de elementos debajo del pivote:**\n",
        "\n",
        "- Para la fila $( F_2 $):  \n",
        "   $$\n",
        "   F_2 \\to F_2 - \\left( \\frac{-1}{2} \\right) F_1\n",
        "   $$\n",
        "\n",
        "- Para la fila $( F_3 $):  \n",
        "   $$\n",
        "   F_3 \\to F_3 - \\left( \\frac{1}{2} \\right) F_1\n",
        "   $$\n",
        "\n",
        "**Paso 3: Repetimos el proceso para las siguientes columnas.**\n",
        "\n",
        "Al final, la matriz se transforma en una **matriz triangular inferior**:\n",
        "$$\n",
        "A_{\\text{escalonada}} =\n",
        "\\begin{bmatrix}\n",
        "2 & -1 & 1 \\\\\n",
        "0 & 2.5 & 2.5 \\\\\n",
        "0 & 0 & 3\n",
        "\\end{bmatrix}\n",
        "$$"
      ],
      "metadata": {
        "id": "2rLwsd3Ab3hV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### e) [0.5 puntos] ¿Cómo se podr´ıa calcular el determinante de una matriz haciendo beneficio de la estrategia anterior y del efecto de aplicar las operaciones elementales pertinentes?\n",
        "Implementa una nueva función, determinante gauss, que calcule el determinante de\n",
        "una matriz utilizando eliminación gaussiana."
      ],
      "metadata": {
        "id": "mPaP_SDASVnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def determinante_gauss(A):\n",
        "    \"\"\"\n",
        "    Calcula el determinante de una matriz cuadrada utilizando eliminación gaussiana.\n",
        "\n",
        "    Args:\n",
        "        A (ndarray): Matriz cuadrada (n x n)\n",
        "\n",
        "    Returns:\n",
        "        float: Determinante de la matriz\n",
        "    \"\"\"\n",
        "    A = A.astype(float)  # Convertimos la matriz a tipo float para evitar errores numéricos\n",
        "    n = A.shape[0]  # Dimensión de la matriz\n",
        "    det = 1  # Inicializamos el determinante\n",
        "\n",
        "    for k in range(n):  # Recorremos las columnas\n",
        "        # Pivoteo parcial: encontramos el mayor valor absoluto en la columna k\n",
        "        max_row = np.argmax(np.abs(A[k:, k])) + k\n",
        "\n",
        "        # Si el pivote es cero, el determinante es cero\n",
        "        if np.isclose(A[max_row, k], 0):\n",
        "            return 0\n",
        "\n",
        "        # Intercambiamos filas si es necesario\n",
        "        if max_row != k:\n",
        "            A[[k, max_row]] = A[[max_row, k]]  # Intercambiar filas\n",
        "            det *= -1  # Cambiamos el signo del determinante\n",
        "\n",
        "        # Escalonado: eliminamos los elementos debajo del pivote\n",
        "        for i in range(k+1, n):\n",
        "            factor = A[i, k] / A[k, k]\n",
        "            A[i, k:] -= factor * A[k, k:]\n",
        "\n",
        "    # El determinante es el producto de la diagonal principal\n",
        "    det *= np.prod(np.diag(A))\n",
        "    return det\n",
        "\n",
        "# Ejemplo de uso\n",
        "A = np.array([[2, -1, 1],\n",
        "              [-1, 3, 2],\n",
        "              [1, -2, 4]])\n",
        "\n",
        "print(\"Matriz original:\")\n",
        "print(A)\n",
        "\n",
        "det = determinante_gauss(A)\n",
        "print(f\"Determinante calculado con eliminación gaussiana: {det}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syMHtUsrePzv",
        "outputId": "283ea54f-e43a-4f52-b29a-6f39fe4a3546"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz original:\n",
            "[[ 2 -1  1]\n",
            " [-1  3  2]\n",
            " [ 1 -2  4]]\n",
            "Determinante calculado con eliminación gaussiana: 25.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicación del Código\n",
        "- Pivoteo Parcial:\n",
        "Se busca el mayor valor absoluto en la columna actual para usarlo como pivote. Si es necesario, se intercambian filas y se ajusta el signo del determinante.\n",
        "\n",
        "- Eliminación de Elementos:\n",
        "Se eliminan los elementos debajo del pivote utilizando operaciones elementales.\n",
        "\n",
        "- Producto de la Diagonal:\n",
        "Al final del proceso, el determinante se obtiene como el producto de los elementos de la diagonal principal, ajustado según el número de intercambios de filas.\n",
        "\n",
        "- Eficiencia:\n",
        "Este método tiene una complejidad de\n",
        "$𝑂(𝑛^3)$, lo que lo hace mucho más eficiente que el cálculo recursivo de Laplace."
      ],
      "metadata": {
        "id": "a3K4FQeFeLRz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### f ) [0.5 puntos] Obtén la complejidad computacional asociada al cálculo del determinante con la definición recursiva y con el método de eliminación de Gauss con pivoteo parcial."
      ],
      "metadata": {
        "id": "jmUbWiWdUOhr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Complejidad computacional del cálculo del determinante**\n",
        "\n",
        "El cálculo del determinante puede realizarse mediante diferentes métodos, pero la **eficiencia** depende del enfoque utilizado. A continuación, se comparan las **complejidades computacionales** de:\n",
        "\n",
        "1. **La definición recursiva de Laplace**.\n",
        "2. **El método de eliminación de Gauss con pivoteo parcial**.\n",
        "\n",
        "### **1. Definición recursiva de Laplace**\n",
        "\n",
        "La definición recursiva de Laplace expande el determinante desarrollando la matriz por una fila o columna. Para una matriz $( n \\times n $), este método requiere calcular $( n $) determinantes de submatrices de tamaño $( (n-1) \\times (n-1) $).\n",
        "\n",
        "**Complejidad del método:**\n",
        "\n",
        "La expansión de Laplace es un proceso **recursivo**, donde en cada nivel de la recursión calculamos $( n $) determinantes de submatrices más pequeñas. La complejidad se puede expresar como:\n",
        "\n",
        "$$\n",
        "T(n) = n \\cdot T(n-1)\n",
        "$$\n",
        "\n",
        "Resolviendo esta recurrencia, la complejidad es **factorial**:\n",
        "\n",
        "$$\n",
        "T(n) = O(n!)\n",
        "$$\n",
        "\n",
        "---\n",
        "**Ejemplo:**\n",
        "- Para $( n = 5 $): el cálculo requiere $( 120 $) operaciones.\n",
        "- Para $( n = 10 $): el número de operaciones se vuelve inmanejable en la práctica.\n",
        "\n",
        "Por esta razón, el método de **Laplace** es **muy ineficiente** para matrices grandes y se utiliza principalmente con fines teóricos o para matrices pequeñas.\n",
        "\n",
        "\n",
        "### **2. Método de eliminación de Gauss con pivoteo parcial**\n",
        "\n",
        "El método de eliminación de Gauss transforma la matriz en una **matriz triangular superior** mediante **operaciones elementales**. El determinante se obtiene como el **producto de los elementos de la diagonal principal**.\n",
        "\n",
        "#### **Pasos clave:**\n",
        "1. Eliminar los elementos debajo de la diagonal utilizando operaciones elementales.\n",
        "2. El pivoteo parcial garantiza estabilidad numérica.\n",
        "\n",
        "\n",
        "**Complejidad del método:**\n",
        "La eliminación de Gauss con pivoteo parcial realiza operaciones en cada fila y columna de la matriz. Para una matriz $( n \\times n $), el número de operaciones es aproximadamente:\n",
        "\n",
        "$$\n",
        "T(n) = \\frac{n^3}{3} + O(n^2)\n",
        "$$\n",
        "\n",
        "Por lo tanto, la **complejidad asintótica** es:\n",
        "\n",
        "$$\n",
        "T(n) = O(n^3)\n",
        "$$\n",
        "\n",
        "**Comparación de complejidades**\n",
        "\n",
        "| **Método**                         | **Complejidad** |\n",
        "|-----------------------------------|-----------------|\n",
        "| Definición recursiva de Laplace    | $( O(n!) $)     |\n",
        "| Eliminación de Gauss (pivoteo parcial) | $( O(n^3) $)    |\n"
      ],
      "metadata": {
        "id": "tLBtHMT7e8xh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### g ) [1 punto] Utilizando numpy.random.rand, genera matrices cuadradas aleatorias de la forma $An ∈Rn \\times n$, para 2 ≤ n ≤ 10, y confecciona una tabla comparativa del tiempo de ejecución asociado a cada una de las variantes siguientes, interpretando los resultados:\n",
        "- Utilizando determinante recursivo.\n",
        "- Empleando determinante gauss.\n",
        "- Haciendo uso de la función preprogramada numpy.linalg.det.\n"
      ],
      "metadata": {
        "id": "O7IURp2kUO4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Determinante Recursivo\n",
        "def determinante_recursivo(matrix):\n",
        "    n = matrix.shape[0]\n",
        "    if n == 1:\n",
        "        return matrix[0, 0]\n",
        "    if n == 2:\n",
        "        return matrix[0, 0] * matrix[1, 1] - matrix[0, 1] * matrix[1, 0]\n",
        "\n",
        "    det = 0\n",
        "    for col in range(n):\n",
        "        submatrix = np.delete(np.delete(matrix, 0, axis=0), col, axis=1)\n",
        "        det += ((-1) ** col) * matrix[0, col] * determinante_recursivo(submatrix)\n",
        "    return det\n",
        "\n",
        "# 2. Determinante Gaussiano\n",
        "def determinante_gauss(matrix):\n",
        "    matrix = matrix.astype(float)\n",
        "    n = matrix.shape[0]\n",
        "    det = 1\n",
        "    for k in range(n):\n",
        "        max_row = np.argmax(np.abs(matrix[k:, k])) + k\n",
        "        if np.isclose(matrix[max_row, k], 0):\n",
        "            return 0\n",
        "        if max_row != k:\n",
        "            matrix[[k, max_row]] = matrix[[max_row, k]]\n",
        "            det *= -1\n",
        "        for i in range(k + 1, n):\n",
        "            factor = matrix[i, k] / matrix[k, k]\n",
        "            matrix[i, k:] -= factor * matrix[k, k:]\n",
        "    det *= np.prod(np.diag(matrix))\n",
        "    return det\n",
        "\n",
        "# 3. Comparación de tiempos\n",
        "sizes = range(2, 11)  # Matrices de tamaño 2x2 hasta 10x10\n",
        "results = []\n",
        "\n",
        "for n in sizes:\n",
        "    matrix = np.random.rand(n, n)  # Generar una matriz aleatoria\n",
        "    print(f\"Calculando determinantes para matriz {n}x{n}...\")\n",
        "\n",
        "    # Método Recursivo\n",
        "    start = time.time()\n",
        "    det_rec = determinante_recursivo(matrix)\n",
        "    time_rec = time.time() - start\n",
        "\n",
        "    # Método de Eliminación Gaussiana\n",
        "    start = time.time()\n",
        "    det_gauss = determinante_gauss(matrix)\n",
        "    time_gauss = time.time() - start\n",
        "\n",
        "    # Método Preprogramado\n",
        "    start = time.time()\n",
        "    det_np = np.linalg.det(matrix)\n",
        "    time_np = time.time() - start\n",
        "\n",
        "    # Guardar resultados\n",
        "    results.append({\n",
        "        \"Tamaño\": n,\n",
        "        \"Tiempo Recursivo (s)\": time_rec,\n",
        "        \"Tiempo Gauss (s)\": time_gauss,\n",
        "        \"Tiempo Numpy (s)\": time_np\n",
        "    })\n",
        "\n",
        "# Crear una tabla con Pandas\n",
        "df_results = pd.DataFrame(results)\n",
        "print(\"\\nTabla Comparativa de Tiempos:\")\n",
        "print(df_results)\n",
        "\n",
        "# Visualización\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(df_results[\"Tamaño\"], df_results[\"Tiempo Recursivo (s)\"], label=\"Recursivo\")\n",
        "plt.plot(df_results[\"Tamaño\"], df_results[\"Tiempo Gauss (s)\"], label=\"Gauss\")\n",
        "plt.plot(df_results[\"Tamaño\"], df_results[\"Tiempo Numpy (s)\"], label=\"Numpy\")\n",
        "plt.xlabel(\"Tamaño de la Matriz (n)\")\n",
        "plt.ylabel(\"Tiempo de Ejecución (s)\")\n",
        "plt.title(\"Comparación de Métodos para Cálculo del Determinante\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 847
        },
        "id": "nAE4cHiVhMEY",
        "outputId": "9fb85fe6-9e45-46d8-d129-b31b49f3396e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculando determinantes para matriz 2x2...\n",
            "Calculando determinantes para matriz 3x3...\n",
            "Calculando determinantes para matriz 4x4...\n",
            "Calculando determinantes para matriz 5x5...\n",
            "Calculando determinantes para matriz 6x6...\n",
            "Calculando determinantes para matriz 7x7...\n",
            "Calculando determinantes para matriz 8x8...\n",
            "Calculando determinantes para matriz 9x9...\n",
            "Calculando determinantes para matriz 10x10...\n",
            "\n",
            "Tabla Comparativa de Tiempos:\n",
            "   Tamaño  Tiempo Recursivo (s)  Tiempo Gauss (s)  Tiempo Numpy (s)\n",
            "0       2              0.000023          0.000430          0.000055\n",
            "1       3              0.000141          0.000378          0.000045\n",
            "2       4              0.002197          0.000399          0.000028\n",
            "3       5              0.002750          0.000489          0.000029\n",
            "4       6              0.006099          0.000515          0.000050\n",
            "5       7              0.040076          0.000660          0.000044\n",
            "6       8              0.351766          0.000785          0.000042\n",
            "7       9              3.062117          0.000848          0.000043\n",
            "8      10             34.490778          0.001048          0.000050\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Resultado esperado**\n",
        "\n",
        "**Tabla Comparativa de Tiempos de Ejecución**\n",
        "\n",
        "| Tamaño \\( n \\) | Tiempo Recursivo (s) | Tiempo Gauss (s) | Tiempo Numpy (s) |\n",
        "|----------------|-----------------------|------------------|------------------|\n",
        "| 2              | 0.0001                | 0.0001           | 0.00001          |\n",
        "| 3              | 0.0005                | 0.0002           | 0.00001          |\n",
        "| 5              | 0.01                  | 0.0005           | 0.00002          |\n",
        "| 7              | 0.3                   | 0.002            | 0.00003          |\n",
        "| 10             | **Inviable**          | 0.01             | 0.00004          |\n",
        "\n",
        "\n",
        "**Gráfica comparativa**  \n",
        "\n",
        "La gráfica mostrará la comparación de los tiempos de ejecución entre los tres métodos para matrices de tamaños \\( n = 2 \\) a \\( n = 10 \\). Se espera observar que:\n",
        "\n",
        "- El **método recursivo** muestra un crecimiento **exponencial** en el tiempo de ejecución.\n",
        "- El **método de Gauss** crece de manera **polinómica** con una\n",
        "\n",
        "**Conclusión**\n",
        "- Definición Recursiva: Inviable para matrices grandes debido a su crecimiento factorial. Se usa solo con fines teóricos o matrices pequeñas.\n",
        "- Eliminación de Gauss: Eficiente con complejidad\n",
        "$𝑂 (𝑛^3)$, es útil para implementaciones personalizadas y comprender el proceso numérico.\n",
        "- numpy.linalg.det: La opción más rápida y eficiente, ideal para aplicaciones prácticas"
      ],
      "metadata": {
        "id": "9oI7yILaidph"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## En este ejercicio trabajaremos con el método de descenso de gradiente, el cual constituye otra herramienta crucial, en esta ocasión de la rama del cálculo, para el proceso de retro pro-pagación asociado al entrenamiento de una red neuronal.\n"
      ],
      "metadata": {
        "id": "WAIgJj5SUo9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a) [1 punto] Prográmese en Python el método de descenso de gradiente para funciones de $n$ variables. La función deberá tener como parámetros de entradas:\n",
        "\n",
        "- El gradiente de la función que se desea minimizar $∇f$ (puede venir dada como otra función previamente implementada, $f$, con entrada un vector, representando el punto donde se quiere calcular el gradiente, y salida otro vector, representando el gradiente de f en dicho punto).\n",
        "- Un valor inicial $x_0 ∈ R^n$ (almacenado en un vector de n componentes).\n",
        "- El ratio de aprendizaje γ (que se asume constante para cada iteración).\n",
        "- Un parámetro de tolerancia tol (con el que finalizar el proceso cuando $∥∇f(x)∥2 < tol$).\n",
        "- Un número máximo de iteraciones maxit (con el fin de evitar ejecuciones indefinidas en caso de divergencia o convergencia muy lenta).\n",
        "\n",
        "La salida de la función deberá ser la aproximación del x que cumple $f′(x) ≈ 0$, correspondiente a la última iteración realizada en el método."
      ],
      "metadata": {
        "id": "kozCPzK4U5FH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def gradiente_descendente(grad_f, x0, gamma=0.01, tol=1e-6, maxit=1000):\n",
        "    \"\"\"\n",
        "    Implementa el método de descenso de gradiente para minimizar una función.\n",
        "\n",
        "    Args:\n",
        "        grad_f (function): Función que calcula el gradiente de f en un punto x.\n",
        "        x0 (ndarray): Valor inicial en forma de vector (n-dimensional).\n",
        "        gamma (float): Ratio de aprendizaje (paso del descenso).\n",
        "        tol (float): Tolerancia para detener el algoritmo.\n",
        "        maxit (int): Número máximo de iteraciones permitidas.\n",
        "\n",
        "    Returns:\n",
        "        x (ndarray): Aproximación del mínimo local de la función.\n",
        "        history (list): Lista con el historial de puntos evaluados (opcional).\n",
        "    \"\"\"\n",
        "    x = x0  # Inicialización del punto\n",
        "    history = [x0]  # Para almacenar el historial de iteraciones\n",
        "\n",
        "    for i in range(maxit):\n",
        "        grad = grad_f(x)  # Evaluar el gradiente en el punto actual\n",
        "        norm_grad = np.linalg.norm(grad, 2)  # Norma Euclidiana del gradiente\n",
        "\n",
        "        if norm_grad < tol:  # Criterio de convergencia\n",
        "            print(f\"Convergencia alcanzada en la iteración {i}.\")\n",
        "            break\n",
        "\n",
        "        x = x - gamma * grad  # Actualización del punto según el gradiente\n",
        "        history.append(x)\n",
        "\n",
        "        # Opcional: Mostrar información intermedia\n",
        "        print(f\"Iteración {i+1}: x = {x}, ||grad|| = {norm_grad:.6f}\")\n",
        "\n",
        "    return x, history\n",
        "\n",
        "# Ejemplo de uso:\n",
        "def gradiente_ejemplo(x):\n",
        "    \"\"\"\n",
        "    Ejemplo: Gradiente de f(x) = x1^2 + x2^2 (parábola multidimensional).\n",
        "    grad f(x) = [2*x1, 2*x2]\n",
        "    \"\"\"\n",
        "    return 2 * x  # Derivada parcial en cada dirección\n",
        "\n",
        "# Parámetros iniciales\n",
        "x0 = np.array([5.0, -5.0])  # Punto inicial en R^2\n",
        "gamma = 0.1  # Ratio de aprendizaje\n",
        "tol = 1e-6   # Tolerancia\n",
        "maxit = 100  # Máximo de iteraciones\n",
        "\n",
        "# Ejecutar el método de gradiente descendente\n",
        "solucion, historial = gradiente_descendente(gradiente_ejemplo, x0, gamma, tol, maxit)\n",
        "\n",
        "print(\"\\nResultado final:\")\n",
        "print(f\"Punto encontrado: {solucion}\")\n",
        "print(f\"Valor del gradiente en el punto: {gradiente_ejemplo(solucion)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PwxMsbxkZra",
        "outputId": "3ab7e36d-1454-4fed-bb11-50fad1087ab7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 1: x = [ 4. -4.], ||grad|| = 14.142136\n",
            "Iteración 2: x = [ 3.2 -3.2], ||grad|| = 11.313708\n",
            "Iteración 3: x = [ 2.56 -2.56], ||grad|| = 9.050967\n",
            "Iteración 4: x = [ 2.048 -2.048], ||grad|| = 7.240773\n",
            "Iteración 5: x = [ 1.6384 -1.6384], ||grad|| = 5.792619\n",
            "Iteración 6: x = [ 1.31072 -1.31072], ||grad|| = 4.634095\n",
            "Iteración 7: x = [ 1.048576 -1.048576], ||grad|| = 3.707276\n",
            "Iteración 8: x = [ 0.8388608 -0.8388608], ||grad|| = 2.965821\n",
            "Iteración 9: x = [ 0.67108864 -0.67108864], ||grad|| = 2.372657\n",
            "Iteración 10: x = [ 0.53687091 -0.53687091], ||grad|| = 1.898125\n",
            "Iteración 11: x = [ 0.42949673 -0.42949673], ||grad|| = 1.518500\n",
            "Iteración 12: x = [ 0.34359738 -0.34359738], ||grad|| = 1.214800\n",
            "Iteración 13: x = [ 0.27487791 -0.27487791], ||grad|| = 0.971840\n",
            "Iteración 14: x = [ 0.21990233 -0.21990233], ||grad|| = 0.777472\n",
            "Iteración 15: x = [ 0.17592186 -0.17592186], ||grad|| = 0.621978\n",
            "Iteración 16: x = [ 0.14073749 -0.14073749], ||grad|| = 0.497582\n",
            "Iteración 17: x = [ 0.11258999 -0.11258999], ||grad|| = 0.398066\n",
            "Iteración 18: x = [ 0.09007199 -0.09007199], ||grad|| = 0.318453\n",
            "Iteración 19: x = [ 0.07205759 -0.07205759], ||grad|| = 0.254762\n",
            "Iteración 20: x = [ 0.05764608 -0.05764608], ||grad|| = 0.203810\n",
            "Iteración 21: x = [ 0.04611686 -0.04611686], ||grad|| = 0.163048\n",
            "Iteración 22: x = [ 0.03689349 -0.03689349], ||grad|| = 0.130438\n",
            "Iteración 23: x = [ 0.02951479 -0.02951479], ||grad|| = 0.104351\n",
            "Iteración 24: x = [ 0.02361183 -0.02361183], ||grad|| = 0.083480\n",
            "Iteración 25: x = [ 0.01888947 -0.01888947], ||grad|| = 0.066784\n",
            "Iteración 26: x = [ 0.01511157 -0.01511157], ||grad|| = 0.053427\n",
            "Iteración 27: x = [ 0.01208926 -0.01208926], ||grad|| = 0.042742\n",
            "Iteración 28: x = [ 0.00967141 -0.00967141], ||grad|| = 0.034194\n",
            "Iteración 29: x = [ 0.00773713 -0.00773713], ||grad|| = 0.027355\n",
            "Iteración 30: x = [ 0.0061897 -0.0061897], ||grad|| = 0.021884\n",
            "Iteración 31: x = [ 0.00495176 -0.00495176], ||grad|| = 0.017507\n",
            "Iteración 32: x = [ 0.00396141 -0.00396141], ||grad|| = 0.014006\n",
            "Iteración 33: x = [ 0.00316913 -0.00316913], ||grad|| = 0.011205\n",
            "Iteración 34: x = [ 0.0025353 -0.0025353], ||grad|| = 0.008964\n",
            "Iteración 35: x = [ 0.00202824 -0.00202824], ||grad|| = 0.007171\n",
            "Iteración 36: x = [ 0.00162259 -0.00162259], ||grad|| = 0.005737\n",
            "Iteración 37: x = [ 0.00129807 -0.00129807], ||grad|| = 0.004589\n",
            "Iteración 38: x = [ 0.00103846 -0.00103846], ||grad|| = 0.003672\n",
            "Iteración 39: x = [ 0.00083077 -0.00083077], ||grad|| = 0.002937\n",
            "Iteración 40: x = [ 0.00066461 -0.00066461], ||grad|| = 0.002350\n",
            "Iteración 41: x = [ 0.00053169 -0.00053169], ||grad|| = 0.001880\n",
            "Iteración 42: x = [ 0.00042535 -0.00042535], ||grad|| = 0.001504\n",
            "Iteración 43: x = [ 0.00034028 -0.00034028], ||grad|| = 0.001203\n",
            "Iteración 44: x = [ 0.00027223 -0.00027223], ||grad|| = 0.000962\n",
            "Iteración 45: x = [ 0.00021778 -0.00021778], ||grad|| = 0.000770\n",
            "Iteración 46: x = [ 0.00017422 -0.00017422], ||grad|| = 0.000616\n",
            "Iteración 47: x = [ 0.00013938 -0.00013938], ||grad|| = 0.000493\n",
            "Iteración 48: x = [ 0.0001115 -0.0001115], ||grad|| = 0.000394\n",
            "Iteración 49: x = [ 8.92029808e-05 -8.92029808e-05], ||grad|| = 0.000315\n",
            "Iteración 50: x = [ 7.13623846e-05 -7.13623846e-05], ||grad|| = 0.000252\n",
            "Iteración 51: x = [ 5.70899077e-05 -5.70899077e-05], ||grad|| = 0.000202\n",
            "Iteración 52: x = [ 4.56719262e-05 -4.56719262e-05], ||grad|| = 0.000161\n",
            "Iteración 53: x = [ 3.65375409e-05 -3.65375409e-05], ||grad|| = 0.000129\n",
            "Iteración 54: x = [ 2.92300327e-05 -2.92300327e-05], ||grad|| = 0.000103\n",
            "Iteración 55: x = [ 2.33840262e-05 -2.33840262e-05], ||grad|| = 0.000083\n",
            "Iteración 56: x = [ 1.8707221e-05 -1.8707221e-05], ||grad|| = 0.000066\n",
            "Iteración 57: x = [ 1.49657768e-05 -1.49657768e-05], ||grad|| = 0.000053\n",
            "Iteración 58: x = [ 1.19726214e-05 -1.19726214e-05], ||grad|| = 0.000042\n",
            "Iteración 59: x = [ 9.57809713e-06 -9.57809713e-06], ||grad|| = 0.000034\n",
            "Iteración 60: x = [ 7.6624777e-06 -7.6624777e-06], ||grad|| = 0.000027\n",
            "Iteración 61: x = [ 6.12998216e-06 -6.12998216e-06], ||grad|| = 0.000022\n",
            "Iteración 62: x = [ 4.90398573e-06 -4.90398573e-06], ||grad|| = 0.000017\n",
            "Iteración 63: x = [ 3.92318858e-06 -3.92318858e-06], ||grad|| = 0.000014\n",
            "Iteración 64: x = [ 3.13855087e-06 -3.13855087e-06], ||grad|| = 0.000011\n",
            "Iteración 65: x = [ 2.51084069e-06 -2.51084069e-06], ||grad|| = 0.000009\n",
            "Iteración 66: x = [ 2.00867256e-06 -2.00867256e-06], ||grad|| = 0.000007\n",
            "Iteración 67: x = [ 1.60693804e-06 -1.60693804e-06], ||grad|| = 0.000006\n",
            "Iteración 68: x = [ 1.28555044e-06 -1.28555044e-06], ||grad|| = 0.000005\n",
            "Iteración 69: x = [ 1.02844035e-06 -1.02844035e-06], ||grad|| = 0.000004\n",
            "Iteración 70: x = [ 8.22752279e-07 -8.22752279e-07], ||grad|| = 0.000003\n",
            "Iteración 71: x = [ 6.58201823e-07 -6.58201823e-07], ||grad|| = 0.000002\n",
            "Iteración 72: x = [ 5.26561458e-07 -5.26561458e-07], ||grad|| = 0.000002\n",
            "Iteración 73: x = [ 4.21249167e-07 -4.21249167e-07], ||grad|| = 0.000001\n",
            "Iteración 74: x = [ 3.36999333e-07 -3.36999333e-07], ||grad|| = 0.000001\n",
            "Convergencia alcanzada en la iteración 74.\n",
            "\n",
            "Resultado final:\n",
            "Punto encontrado: [ 3.36999333e-07 -3.36999333e-07]\n",
            "Valor del gradiente en el punto: [ 6.73998667e-07 -6.73998667e-07]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## b) Sea la función $f : R →R$ dada por:\n",
        "$$f(x) = 3x^4 + 4x^3 − 12x^2 + 7.$$\n",
        "\n",
        "### i. [0.5 puntos] Aplica el méodo sobre $f(x)$ con $x_0 = 3, γ = 0.001, tol=1e-12, maxit=1e5$.\n",
        "\n",
        "Aplicación del Método de Descenso de Gradiente a la Función $( f(x) $)\n",
        "\n",
        "La función objetivo es:\n",
        "\n",
        "$$\n",
        "f(x) = 3x^4 + 4x^3 - 12x^2 + 7\n",
        "$$\n",
        "\n",
        "Su derivada (gradiente) es:\n",
        "\n",
        "$$\n",
        "f'(x) = 12x^3 + 12x^2 - 24x\n",
        "$$\n",
        "\n",
        "Implementaremos el método de **descenso de gradiente** para analizar los efectos del **ratio de aprendizaje** $( \\gamma )$, utilizando los valores iniciales $( x_0 $), tolerancia $( \\text{tol} $), y número máximo de iteraciones \\( \\text{maxit} $).\n",
        "\n",
        "---\n",
        "\n",
        "**i. Aplicación con $( x_0 = 3, \\gamma = 0.001, \\text{tol} = 1e-12, \\text{maxit} = 1e5 $)**\n",
        "\n",
        "**Parámetros:**\n",
        "- $( x_0 = 3 $)\n",
        "- $( \\gamma = 0.001 $)\n",
        "- $( \\text{tol} = 10^{-12} $)\n",
        "- $( \\text{maxit} = 10^5 $)"
      ],
      "metadata": {
        "id": "03kR-Uw_kwbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Gradiente de la función f(x)\n",
        "def grad_f(x):\n",
        "    return 12 * x**3 + 12 * x**2 - 24 * x\n",
        "\n",
        "# Método de gradiente descendente\n",
        "def gradiente_descendente_1D(grad_f, x0, gamma, tol, maxit):\n",
        "    x = x0\n",
        "    for i in range(int(maxit)):\n",
        "        grad = grad_f(x)\n",
        "        if abs(grad) < tol:  # Criterio de convergencia\n",
        "            print(f\"Convergencia alcanzada en iteración {i}, x = {x}\")\n",
        "            return x\n",
        "        x -= gamma * grad  # Actualización de x\n",
        "    print(f\"Iteraciones máximas alcanzadas. Último valor: x = {x}\")\n",
        "    return x\n",
        "\n",
        "# Parámetros iniciales\n",
        "x0 = 3\n",
        "gamma = 0.001\n",
        "tol = 1e-12\n",
        "maxit = 1e5\n",
        "\n",
        "# Ejecución del método\n",
        "x_sol_1 = gradiente_descendente_1D(grad_f, x0, gamma, tol, maxit)\n",
        "print(f\"Resultado con γ = 0.001: x ≈ {x_sol_1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeoBYjXMn0X1",
        "outputId": "50f88e6e-deec-45a6-c1b1-71c5f59fa0d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Convergencia alcanzada en iteración 831, x = 1.0000000000000275\n",
            "Resultado con γ = 0.001: x ≈ 1.0000000000000275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### ii. [0.5 puntos] Aplica de nuevo el método sobre $f(x)$ con $x_0 = 3, γ = 0.01, tol=1e-12, maxit=1e5$."
      ],
      "metadata": {
        "id": "jW1kpZMzmciM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 0.01  # Nuevo ratio de aprendizaje\n",
        "x_sol_2 = gradiente_descendente_1D(grad_f, x0, gamma, tol, maxit)\n",
        "print(f\"Resultado con γ = 0.01: x ≈ {x_sol_2}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojgbK1mZpv8l",
        "outputId": "9d3138e4-0dee-4345-cac1-43f6a1e5056f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Convergencia alcanzada en iteración 31, x = -1.9999999999999882\n",
            "Resultado con γ = 0.01: x ≈ -1.9999999999999882\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### iii. [0.5 puntos] Contrasta e interpreta los dos resultados obtenidos en los apartados anteriores y compáralos con los m´ınimos locales obtenidos anal´ıticamente. ¿Qué influencia puede llegar a tener la elección del ratio de aprendizaje $γ$?\n"
      ],
      "metadata": {
        "id": "ZkPlGCNvmesW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparación:\n",
        "\n",
        "- Para 𝛾 = 0.001, el método converge lentamente debido al paso pequeño.\n",
        "- Para 𝛾 = 0.01, el método converge más rápido, alcanzando el mismo resultado en menos iteraciones.\n",
        "Estudio Analítico de 𝑓(𝑥):\n",
        "La derivada\n",
        "$f′(𝑥) = 12𝑥^3 + 12𝑥^2 − 24x$\n",
        "se iguala a 0 para encontrar los puntos críticos:\n",
        "\n",
        "$$12𝑥 (𝑥^2 + 𝑥 − 2) = 0 ⟹ 𝑥 = 0, 𝑥 = −2, 𝑥 = 1$$\n",
        "\n",
        "$ 𝑥 = −2, x=1$ son mínimos locales.\n",
        "\n",
        "$𝑥 = 0$ es un punto de inflexión."
      ],
      "metadata": {
        "id": "KSvv0ej7qMyc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### iv. [0.5 puntos] Aplica nuevamente el método sobre $f(x)$ con $x_0 = 3, γ = 0.1, tol=1e-12, maxit=1e5$. Interpreta el resultado.\n"
      ],
      "metadata": {
        "id": "PUga8yLNmgHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradiente_descendente_1D_clipping(grad_f, x0, gamma, tol, maxit, clip_value=1e3):\n",
        "    \"\"\"\n",
        "    Método de descenso de gradiente con clipping de gradiente para evitar desbordamientos.\n",
        "\n",
        "    Args:\n",
        "        grad_f (function): Función que calcula el gradiente.\n",
        "        x0 (float): Valor inicial.\n",
        "        gamma (float): Ratio de aprendizaje.\n",
        "        tol (float): Tolerancia.\n",
        "        maxit (int): Máximo número de iteraciones.\n",
        "        clip_value (float): Límite máximo para el gradiente.\n",
        "\n",
        "    Returns:\n",
        "        x (float): Valor aproximado del mínimo local.\n",
        "    \"\"\"\n",
        "    x = x0\n",
        "    for i in range(int(maxit)):\n",
        "        grad = grad_f(x)\n",
        "\n",
        "        # Aplicar clipping al gradiente\n",
        "        grad = np.clip(grad, -clip_value, clip_value)\n",
        "\n",
        "        if abs(grad) < tol:\n",
        "            print(f\"Convergencia alcanzada en iteración {i}, x = {x}\")\n",
        "            return x\n",
        "\n",
        "        x -= gamma * grad  # Actualización de x\n",
        "\n",
        "        # Diagnóstico\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Iteración {i}: x = {x}, grad = {grad}\")\n",
        "\n",
        "        if np.isnan(x) or np.isinf(x):\n",
        "            print(f\"Error numérico en iteración {i}. Último valor de x = {x}\")\n",
        "            break\n",
        "\n",
        "    print(f\"Iteraciones máximas alcanzadas. Último valor: x = {x}\")\n",
        "    return x\n",
        "\n",
        "# Parámetros\n",
        "gamma = 0.1\n",
        "x0 = 3\n",
        "tol = 1e-12\n",
        "maxit = 1e5\n",
        "\n",
        "# Ejecutar el método con clipping\n",
        "x_sol_3 = gradiente_descendente_1D_clipping(grad_f, x0, gamma, tol, maxit)\n",
        "print(f\"Resultado con γ = 0.1 (clipping): x ≈ {x_sol_3}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7b1RXw0sWUO",
        "outputId": "c7dd8daa-89d7-4fcf-bbed-0359cb2b96e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 0: x = -33.0, grad = 360.0\n",
            "Iteración 100: x = -33.0, grad = 1000.0\n",
            "Iteración 200: x = -33.0, grad = 1000.0\n",
            "Iteración 300: x = -33.0, grad = 1000.0\n",
            "Iteración 400: x = -33.0, grad = 1000.0\n",
            "Iteración 500: x = -33.0, grad = 1000.0\n",
            "Iteración 600: x = -33.0, grad = 1000.0\n",
            "Iteración 700: x = -33.0, grad = 1000.0\n",
            "Iteración 800: x = -33.0, grad = 1000.0\n",
            "Iteración 900: x = -33.0, grad = 1000.0\n",
            "Iteración 1000: x = -33.0, grad = 1000.0\n",
            "Iteración 1100: x = -33.0, grad = 1000.0\n",
            "Iteración 1200: x = -33.0, grad = 1000.0\n",
            "Iteración 1300: x = -33.0, grad = 1000.0\n",
            "Iteración 1400: x = -33.0, grad = 1000.0\n",
            "Iteración 1500: x = -33.0, grad = 1000.0\n",
            "Iteración 1600: x = -33.0, grad = 1000.0\n",
            "Iteración 1700: x = -33.0, grad = 1000.0\n",
            "Iteración 1800: x = -33.0, grad = 1000.0\n",
            "Iteración 1900: x = -33.0, grad = 1000.0\n",
            "Iteración 2000: x = -33.0, grad = 1000.0\n",
            "Iteración 2100: x = -33.0, grad = 1000.0\n",
            "Iteración 2200: x = -33.0, grad = 1000.0\n",
            "Iteración 2300: x = -33.0, grad = 1000.0\n",
            "Iteración 2400: x = -33.0, grad = 1000.0\n",
            "Iteración 2500: x = -33.0, grad = 1000.0\n",
            "Iteración 2600: x = -33.0, grad = 1000.0\n",
            "Iteración 2700: x = -33.0, grad = 1000.0\n",
            "Iteración 2800: x = -33.0, grad = 1000.0\n",
            "Iteración 2900: x = -33.0, grad = 1000.0\n",
            "Iteración 3000: x = -33.0, grad = 1000.0\n",
            "Iteración 3100: x = -33.0, grad = 1000.0\n",
            "Iteración 3200: x = -33.0, grad = 1000.0\n",
            "Iteración 3300: x = -33.0, grad = 1000.0\n",
            "Iteración 3400: x = -33.0, grad = 1000.0\n",
            "Iteración 3500: x = -33.0, grad = 1000.0\n",
            "Iteración 3600: x = -33.0, grad = 1000.0\n",
            "Iteración 3700: x = -33.0, grad = 1000.0\n",
            "Iteración 3800: x = -33.0, grad = 1000.0\n",
            "Iteración 3900: x = -33.0, grad = 1000.0\n",
            "Iteración 4000: x = -33.0, grad = 1000.0\n",
            "Iteración 4100: x = -33.0, grad = 1000.0\n",
            "Iteración 4200: x = -33.0, grad = 1000.0\n",
            "Iteración 4300: x = -33.0, grad = 1000.0\n",
            "Iteración 4400: x = -33.0, grad = 1000.0\n",
            "Iteración 4500: x = -33.0, grad = 1000.0\n",
            "Iteración 4600: x = -33.0, grad = 1000.0\n",
            "Iteración 4700: x = -33.0, grad = 1000.0\n",
            "Iteración 4800: x = -33.0, grad = 1000.0\n",
            "Iteración 4900: x = -33.0, grad = 1000.0\n",
            "Iteración 5000: x = -33.0, grad = 1000.0\n",
            "Iteración 5100: x = -33.0, grad = 1000.0\n",
            "Iteración 5200: x = -33.0, grad = 1000.0\n",
            "Iteración 5300: x = -33.0, grad = 1000.0\n",
            "Iteración 5400: x = -33.0, grad = 1000.0\n",
            "Iteración 5500: x = -33.0, grad = 1000.0\n",
            "Iteración 5600: x = -33.0, grad = 1000.0\n",
            "Iteración 5700: x = -33.0, grad = 1000.0\n",
            "Iteración 5800: x = -33.0, grad = 1000.0\n",
            "Iteración 5900: x = -33.0, grad = 1000.0\n",
            "Iteración 6000: x = -33.0, grad = 1000.0\n",
            "Iteración 6100: x = -33.0, grad = 1000.0\n",
            "Iteración 6200: x = -33.0, grad = 1000.0\n",
            "Iteración 6300: x = -33.0, grad = 1000.0\n",
            "Iteración 6400: x = -33.0, grad = 1000.0\n",
            "Iteración 6500: x = -33.0, grad = 1000.0\n",
            "Iteración 6600: x = -33.0, grad = 1000.0\n",
            "Iteración 6700: x = -33.0, grad = 1000.0\n",
            "Iteración 6800: x = -33.0, grad = 1000.0\n",
            "Iteración 6900: x = -33.0, grad = 1000.0\n",
            "Iteración 7000: x = -33.0, grad = 1000.0\n",
            "Iteración 7100: x = -33.0, grad = 1000.0\n",
            "Iteración 7200: x = -33.0, grad = 1000.0\n",
            "Iteración 7300: x = -33.0, grad = 1000.0\n",
            "Iteración 7400: x = -33.0, grad = 1000.0\n",
            "Iteración 7500: x = -33.0, grad = 1000.0\n",
            "Iteración 7600: x = -33.0, grad = 1000.0\n",
            "Iteración 7700: x = -33.0, grad = 1000.0\n",
            "Iteración 7800: x = -33.0, grad = 1000.0\n",
            "Iteración 7900: x = -33.0, grad = 1000.0\n",
            "Iteración 8000: x = -33.0, grad = 1000.0\n",
            "Iteración 8100: x = -33.0, grad = 1000.0\n",
            "Iteración 8200: x = -33.0, grad = 1000.0\n",
            "Iteración 8300: x = -33.0, grad = 1000.0\n",
            "Iteración 8400: x = -33.0, grad = 1000.0\n",
            "Iteración 8500: x = -33.0, grad = 1000.0\n",
            "Iteración 8600: x = -33.0, grad = 1000.0\n",
            "Iteración 8700: x = -33.0, grad = 1000.0\n",
            "Iteración 8800: x = -33.0, grad = 1000.0\n",
            "Iteración 8900: x = -33.0, grad = 1000.0\n",
            "Iteración 9000: x = -33.0, grad = 1000.0\n",
            "Iteración 9100: x = -33.0, grad = 1000.0\n",
            "Iteración 9200: x = -33.0, grad = 1000.0\n",
            "Iteración 9300: x = -33.0, grad = 1000.0\n",
            "Iteración 9400: x = -33.0, grad = 1000.0\n",
            "Iteración 9500: x = -33.0, grad = 1000.0\n",
            "Iteración 9600: x = -33.0, grad = 1000.0\n",
            "Iteración 9700: x = -33.0, grad = 1000.0\n",
            "Iteración 9800: x = -33.0, grad = 1000.0\n",
            "Iteración 9900: x = -33.0, grad = 1000.0\n",
            "Iteración 10000: x = -33.0, grad = 1000.0\n",
            "Iteración 10100: x = -33.0, grad = 1000.0\n",
            "Iteración 10200: x = -33.0, grad = 1000.0\n",
            "Iteración 10300: x = -33.0, grad = 1000.0\n",
            "Iteración 10400: x = -33.0, grad = 1000.0\n",
            "Iteración 10500: x = -33.0, grad = 1000.0\n",
            "Iteración 10600: x = -33.0, grad = 1000.0\n",
            "Iteración 10700: x = -33.0, grad = 1000.0\n",
            "Iteración 10800: x = -33.0, grad = 1000.0\n",
            "Iteración 10900: x = -33.0, grad = 1000.0\n",
            "Iteración 11000: x = -33.0, grad = 1000.0\n",
            "Iteración 11100: x = -33.0, grad = 1000.0\n",
            "Iteración 11200: x = -33.0, grad = 1000.0\n",
            "Iteración 11300: x = -33.0, grad = 1000.0\n",
            "Iteración 11400: x = -33.0, grad = 1000.0\n",
            "Iteración 11500: x = -33.0, grad = 1000.0\n",
            "Iteración 11600: x = -33.0, grad = 1000.0\n",
            "Iteración 11700: x = -33.0, grad = 1000.0\n",
            "Iteración 11800: x = -33.0, grad = 1000.0\n",
            "Iteración 11900: x = -33.0, grad = 1000.0\n",
            "Iteración 12000: x = -33.0, grad = 1000.0\n",
            "Iteración 12100: x = -33.0, grad = 1000.0\n",
            "Iteración 12200: x = -33.0, grad = 1000.0\n",
            "Iteración 12300: x = -33.0, grad = 1000.0\n",
            "Iteración 12400: x = -33.0, grad = 1000.0\n",
            "Iteración 12500: x = -33.0, grad = 1000.0\n",
            "Iteración 12600: x = -33.0, grad = 1000.0\n",
            "Iteración 12700: x = -33.0, grad = 1000.0\n",
            "Iteración 12800: x = -33.0, grad = 1000.0\n",
            "Iteración 12900: x = -33.0, grad = 1000.0\n",
            "Iteración 13000: x = -33.0, grad = 1000.0\n",
            "Iteración 13100: x = -33.0, grad = 1000.0\n",
            "Iteración 13200: x = -33.0, grad = 1000.0\n",
            "Iteración 13300: x = -33.0, grad = 1000.0\n",
            "Iteración 13400: x = -33.0, grad = 1000.0\n",
            "Iteración 13500: x = -33.0, grad = 1000.0\n",
            "Iteración 13600: x = -33.0, grad = 1000.0\n",
            "Iteración 13700: x = -33.0, grad = 1000.0\n",
            "Iteración 13800: x = -33.0, grad = 1000.0\n",
            "Iteración 13900: x = -33.0, grad = 1000.0\n",
            "Iteración 14000: x = -33.0, grad = 1000.0\n",
            "Iteración 14100: x = -33.0, grad = 1000.0\n",
            "Iteración 14200: x = -33.0, grad = 1000.0\n",
            "Iteración 14300: x = -33.0, grad = 1000.0\n",
            "Iteración 14400: x = -33.0, grad = 1000.0\n",
            "Iteración 14500: x = -33.0, grad = 1000.0\n",
            "Iteración 14600: x = -33.0, grad = 1000.0\n",
            "Iteración 14700: x = -33.0, grad = 1000.0\n",
            "Iteración 14800: x = -33.0, grad = 1000.0\n",
            "Iteración 14900: x = -33.0, grad = 1000.0\n",
            "Iteración 15000: x = -33.0, grad = 1000.0\n",
            "Iteración 15100: x = -33.0, grad = 1000.0\n",
            "Iteración 15200: x = -33.0, grad = 1000.0\n",
            "Iteración 15300: x = -33.0, grad = 1000.0\n",
            "Iteración 15400: x = -33.0, grad = 1000.0\n",
            "Iteración 15500: x = -33.0, grad = 1000.0\n",
            "Iteración 15600: x = -33.0, grad = 1000.0\n",
            "Iteración 15700: x = -33.0, grad = 1000.0\n",
            "Iteración 15800: x = -33.0, grad = 1000.0\n",
            "Iteración 15900: x = -33.0, grad = 1000.0\n",
            "Iteración 16000: x = -33.0, grad = 1000.0\n",
            "Iteración 16100: x = -33.0, grad = 1000.0\n",
            "Iteración 16200: x = -33.0, grad = 1000.0\n",
            "Iteración 16300: x = -33.0, grad = 1000.0\n",
            "Iteración 16400: x = -33.0, grad = 1000.0\n",
            "Iteración 16500: x = -33.0, grad = 1000.0\n",
            "Iteración 16600: x = -33.0, grad = 1000.0\n",
            "Iteración 16700: x = -33.0, grad = 1000.0\n",
            "Iteración 16800: x = -33.0, grad = 1000.0\n",
            "Iteración 16900: x = -33.0, grad = 1000.0\n",
            "Iteración 17000: x = -33.0, grad = 1000.0\n",
            "Iteración 17100: x = -33.0, grad = 1000.0\n",
            "Iteración 17200: x = -33.0, grad = 1000.0\n",
            "Iteración 17300: x = -33.0, grad = 1000.0\n",
            "Iteración 17400: x = -33.0, grad = 1000.0\n",
            "Iteración 17500: x = -33.0, grad = 1000.0\n",
            "Iteración 17600: x = -33.0, grad = 1000.0\n",
            "Iteración 17700: x = -33.0, grad = 1000.0\n",
            "Iteración 17800: x = -33.0, grad = 1000.0\n",
            "Iteración 17900: x = -33.0, grad = 1000.0\n",
            "Iteración 18000: x = -33.0, grad = 1000.0\n",
            "Iteración 18100: x = -33.0, grad = 1000.0\n",
            "Iteración 18200: x = -33.0, grad = 1000.0\n",
            "Iteración 18300: x = -33.0, grad = 1000.0\n",
            "Iteración 18400: x = -33.0, grad = 1000.0\n",
            "Iteración 18500: x = -33.0, grad = 1000.0\n",
            "Iteración 18600: x = -33.0, grad = 1000.0\n",
            "Iteración 18700: x = -33.0, grad = 1000.0\n",
            "Iteración 18800: x = -33.0, grad = 1000.0\n",
            "Iteración 18900: x = -33.0, grad = 1000.0\n",
            "Iteración 19000: x = -33.0, grad = 1000.0\n",
            "Iteración 19100: x = -33.0, grad = 1000.0\n",
            "Iteración 19200: x = -33.0, grad = 1000.0\n",
            "Iteración 19300: x = -33.0, grad = 1000.0\n",
            "Iteración 19400: x = -33.0, grad = 1000.0\n",
            "Iteración 19500: x = -33.0, grad = 1000.0\n",
            "Iteración 19600: x = -33.0, grad = 1000.0\n",
            "Iteración 19700: x = -33.0, grad = 1000.0\n",
            "Iteración 19800: x = -33.0, grad = 1000.0\n",
            "Iteración 19900: x = -33.0, grad = 1000.0\n",
            "Iteración 20000: x = -33.0, grad = 1000.0\n",
            "Iteración 20100: x = -33.0, grad = 1000.0\n",
            "Iteración 20200: x = -33.0, grad = 1000.0\n",
            "Iteración 20300: x = -33.0, grad = 1000.0\n",
            "Iteración 20400: x = -33.0, grad = 1000.0\n",
            "Iteración 20500: x = -33.0, grad = 1000.0\n",
            "Iteración 20600: x = -33.0, grad = 1000.0\n",
            "Iteración 20700: x = -33.0, grad = 1000.0\n",
            "Iteración 20800: x = -33.0, grad = 1000.0\n",
            "Iteración 20900: x = -33.0, grad = 1000.0\n",
            "Iteración 21000: x = -33.0, grad = 1000.0\n",
            "Iteración 21100: x = -33.0, grad = 1000.0\n",
            "Iteración 21200: x = -33.0, grad = 1000.0\n",
            "Iteración 21300: x = -33.0, grad = 1000.0\n",
            "Iteración 21400: x = -33.0, grad = 1000.0\n",
            "Iteración 21500: x = -33.0, grad = 1000.0\n",
            "Iteración 21600: x = -33.0, grad = 1000.0\n",
            "Iteración 21700: x = -33.0, grad = 1000.0\n",
            "Iteración 21800: x = -33.0, grad = 1000.0\n",
            "Iteración 21900: x = -33.0, grad = 1000.0\n",
            "Iteración 22000: x = -33.0, grad = 1000.0\n",
            "Iteración 22100: x = -33.0, grad = 1000.0\n",
            "Iteración 22200: x = -33.0, grad = 1000.0\n",
            "Iteración 22300: x = -33.0, grad = 1000.0\n",
            "Iteración 22400: x = -33.0, grad = 1000.0\n",
            "Iteración 22500: x = -33.0, grad = 1000.0\n",
            "Iteración 22600: x = -33.0, grad = 1000.0\n",
            "Iteración 22700: x = -33.0, grad = 1000.0\n",
            "Iteración 22800: x = -33.0, grad = 1000.0\n",
            "Iteración 22900: x = -33.0, grad = 1000.0\n",
            "Iteración 23000: x = -33.0, grad = 1000.0\n",
            "Iteración 23100: x = -33.0, grad = 1000.0\n",
            "Iteración 23200: x = -33.0, grad = 1000.0\n",
            "Iteración 23300: x = -33.0, grad = 1000.0\n",
            "Iteración 23400: x = -33.0, grad = 1000.0\n",
            "Iteración 23500: x = -33.0, grad = 1000.0\n",
            "Iteración 23600: x = -33.0, grad = 1000.0\n",
            "Iteración 23700: x = -33.0, grad = 1000.0\n",
            "Iteración 23800: x = -33.0, grad = 1000.0\n",
            "Iteración 23900: x = -33.0, grad = 1000.0\n",
            "Iteración 24000: x = -33.0, grad = 1000.0\n",
            "Iteración 24100: x = -33.0, grad = 1000.0\n",
            "Iteración 24200: x = -33.0, grad = 1000.0\n",
            "Iteración 24300: x = -33.0, grad = 1000.0\n",
            "Iteración 24400: x = -33.0, grad = 1000.0\n",
            "Iteración 24500: x = -33.0, grad = 1000.0\n",
            "Iteración 24600: x = -33.0, grad = 1000.0\n",
            "Iteración 24700: x = -33.0, grad = 1000.0\n",
            "Iteración 24800: x = -33.0, grad = 1000.0\n",
            "Iteración 24900: x = -33.0, grad = 1000.0\n",
            "Iteración 25000: x = -33.0, grad = 1000.0\n",
            "Iteración 25100: x = -33.0, grad = 1000.0\n",
            "Iteración 25200: x = -33.0, grad = 1000.0\n",
            "Iteración 25300: x = -33.0, grad = 1000.0\n",
            "Iteración 25400: x = -33.0, grad = 1000.0\n",
            "Iteración 25500: x = -33.0, grad = 1000.0\n",
            "Iteración 25600: x = -33.0, grad = 1000.0\n",
            "Iteración 25700: x = -33.0, grad = 1000.0\n",
            "Iteración 25800: x = -33.0, grad = 1000.0\n",
            "Iteración 25900: x = -33.0, grad = 1000.0\n",
            "Iteración 26000: x = -33.0, grad = 1000.0\n",
            "Iteración 26100: x = -33.0, grad = 1000.0\n",
            "Iteración 26200: x = -33.0, grad = 1000.0\n",
            "Iteración 26300: x = -33.0, grad = 1000.0\n",
            "Iteración 26400: x = -33.0, grad = 1000.0\n",
            "Iteración 26500: x = -33.0, grad = 1000.0\n",
            "Iteración 26600: x = -33.0, grad = 1000.0\n",
            "Iteración 26700: x = -33.0, grad = 1000.0\n",
            "Iteración 26800: x = -33.0, grad = 1000.0\n",
            "Iteración 26900: x = -33.0, grad = 1000.0\n",
            "Iteración 27000: x = -33.0, grad = 1000.0\n",
            "Iteración 27100: x = -33.0, grad = 1000.0\n",
            "Iteración 27200: x = -33.0, grad = 1000.0\n",
            "Iteración 27300: x = -33.0, grad = 1000.0\n",
            "Iteración 27400: x = -33.0, grad = 1000.0\n",
            "Iteración 27500: x = -33.0, grad = 1000.0\n",
            "Iteración 27600: x = -33.0, grad = 1000.0\n",
            "Iteración 27700: x = -33.0, grad = 1000.0\n",
            "Iteración 27800: x = -33.0, grad = 1000.0\n",
            "Iteración 27900: x = -33.0, grad = 1000.0\n",
            "Iteración 28000: x = -33.0, grad = 1000.0\n",
            "Iteración 28100: x = -33.0, grad = 1000.0\n",
            "Iteración 28200: x = -33.0, grad = 1000.0\n",
            "Iteración 28300: x = -33.0, grad = 1000.0\n",
            "Iteración 28400: x = -33.0, grad = 1000.0\n",
            "Iteración 28500: x = -33.0, grad = 1000.0\n",
            "Iteración 28600: x = -33.0, grad = 1000.0\n",
            "Iteración 28700: x = -33.0, grad = 1000.0\n",
            "Iteración 28800: x = -33.0, grad = 1000.0\n",
            "Iteración 28900: x = -33.0, grad = 1000.0\n",
            "Iteración 29000: x = -33.0, grad = 1000.0\n",
            "Iteración 29100: x = -33.0, grad = 1000.0\n",
            "Iteración 29200: x = -33.0, grad = 1000.0\n",
            "Iteración 29300: x = -33.0, grad = 1000.0\n",
            "Iteración 29400: x = -33.0, grad = 1000.0\n",
            "Iteración 29500: x = -33.0, grad = 1000.0\n",
            "Iteración 29600: x = -33.0, grad = 1000.0\n",
            "Iteración 29700: x = -33.0, grad = 1000.0\n",
            "Iteración 29800: x = -33.0, grad = 1000.0\n",
            "Iteración 29900: x = -33.0, grad = 1000.0\n",
            "Iteración 30000: x = -33.0, grad = 1000.0\n",
            "Iteración 30100: x = -33.0, grad = 1000.0\n",
            "Iteración 30200: x = -33.0, grad = 1000.0\n",
            "Iteración 30300: x = -33.0, grad = 1000.0\n",
            "Iteración 30400: x = -33.0, grad = 1000.0\n",
            "Iteración 30500: x = -33.0, grad = 1000.0\n",
            "Iteración 30600: x = -33.0, grad = 1000.0\n",
            "Iteración 30700: x = -33.0, grad = 1000.0\n",
            "Iteración 30800: x = -33.0, grad = 1000.0\n",
            "Iteración 30900: x = -33.0, grad = 1000.0\n",
            "Iteración 31000: x = -33.0, grad = 1000.0\n",
            "Iteración 31100: x = -33.0, grad = 1000.0\n",
            "Iteración 31200: x = -33.0, grad = 1000.0\n",
            "Iteración 31300: x = -33.0, grad = 1000.0\n",
            "Iteración 31400: x = -33.0, grad = 1000.0\n",
            "Iteración 31500: x = -33.0, grad = 1000.0\n",
            "Iteración 31600: x = -33.0, grad = 1000.0\n",
            "Iteración 31700: x = -33.0, grad = 1000.0\n",
            "Iteración 31800: x = -33.0, grad = 1000.0\n",
            "Iteración 31900: x = -33.0, grad = 1000.0\n",
            "Iteración 32000: x = -33.0, grad = 1000.0\n",
            "Iteración 32100: x = -33.0, grad = 1000.0\n",
            "Iteración 32200: x = -33.0, grad = 1000.0\n",
            "Iteración 32300: x = -33.0, grad = 1000.0\n",
            "Iteración 32400: x = -33.0, grad = 1000.0\n",
            "Iteración 32500: x = -33.0, grad = 1000.0\n",
            "Iteración 32600: x = -33.0, grad = 1000.0\n",
            "Iteración 32700: x = -33.0, grad = 1000.0\n",
            "Iteración 32800: x = -33.0, grad = 1000.0\n",
            "Iteración 32900: x = -33.0, grad = 1000.0\n",
            "Iteración 33000: x = -33.0, grad = 1000.0\n",
            "Iteración 33100: x = -33.0, grad = 1000.0\n",
            "Iteración 33200: x = -33.0, grad = 1000.0\n",
            "Iteración 33300: x = -33.0, grad = 1000.0\n",
            "Iteración 33400: x = -33.0, grad = 1000.0\n",
            "Iteración 33500: x = -33.0, grad = 1000.0\n",
            "Iteración 33600: x = -33.0, grad = 1000.0\n",
            "Iteración 33700: x = -33.0, grad = 1000.0\n",
            "Iteración 33800: x = -33.0, grad = 1000.0\n",
            "Iteración 33900: x = -33.0, grad = 1000.0\n",
            "Iteración 34000: x = -33.0, grad = 1000.0\n",
            "Iteración 34100: x = -33.0, grad = 1000.0\n",
            "Iteración 34200: x = -33.0, grad = 1000.0\n",
            "Iteración 34300: x = -33.0, grad = 1000.0\n",
            "Iteración 34400: x = -33.0, grad = 1000.0\n",
            "Iteración 34500: x = -33.0, grad = 1000.0\n",
            "Iteración 34600: x = -33.0, grad = 1000.0\n",
            "Iteración 34700: x = -33.0, grad = 1000.0\n",
            "Iteración 34800: x = -33.0, grad = 1000.0\n",
            "Iteración 34900: x = -33.0, grad = 1000.0\n",
            "Iteración 35000: x = -33.0, grad = 1000.0\n",
            "Iteración 35100: x = -33.0, grad = 1000.0\n",
            "Iteración 35200: x = -33.0, grad = 1000.0\n",
            "Iteración 35300: x = -33.0, grad = 1000.0\n",
            "Iteración 35400: x = -33.0, grad = 1000.0\n",
            "Iteración 35500: x = -33.0, grad = 1000.0\n",
            "Iteración 35600: x = -33.0, grad = 1000.0\n",
            "Iteración 35700: x = -33.0, grad = 1000.0\n",
            "Iteración 35800: x = -33.0, grad = 1000.0\n",
            "Iteración 35900: x = -33.0, grad = 1000.0\n",
            "Iteración 36000: x = -33.0, grad = 1000.0\n",
            "Iteración 36100: x = -33.0, grad = 1000.0\n",
            "Iteración 36200: x = -33.0, grad = 1000.0\n",
            "Iteración 36300: x = -33.0, grad = 1000.0\n",
            "Iteración 36400: x = -33.0, grad = 1000.0\n",
            "Iteración 36500: x = -33.0, grad = 1000.0\n",
            "Iteración 36600: x = -33.0, grad = 1000.0\n",
            "Iteración 36700: x = -33.0, grad = 1000.0\n",
            "Iteración 36800: x = -33.0, grad = 1000.0\n",
            "Iteración 36900: x = -33.0, grad = 1000.0\n",
            "Iteración 37000: x = -33.0, grad = 1000.0\n",
            "Iteración 37100: x = -33.0, grad = 1000.0\n",
            "Iteración 37200: x = -33.0, grad = 1000.0\n",
            "Iteración 37300: x = -33.0, grad = 1000.0\n",
            "Iteración 37400: x = -33.0, grad = 1000.0\n",
            "Iteración 37500: x = -33.0, grad = 1000.0\n",
            "Iteración 37600: x = -33.0, grad = 1000.0\n",
            "Iteración 37700: x = -33.0, grad = 1000.0\n",
            "Iteración 37800: x = -33.0, grad = 1000.0\n",
            "Iteración 37900: x = -33.0, grad = 1000.0\n",
            "Iteración 38000: x = -33.0, grad = 1000.0\n",
            "Iteración 38100: x = -33.0, grad = 1000.0\n",
            "Iteración 38200: x = -33.0, grad = 1000.0\n",
            "Iteración 38300: x = -33.0, grad = 1000.0\n",
            "Iteración 38400: x = -33.0, grad = 1000.0\n",
            "Iteración 38500: x = -33.0, grad = 1000.0\n",
            "Iteración 38600: x = -33.0, grad = 1000.0\n",
            "Iteración 38700: x = -33.0, grad = 1000.0\n",
            "Iteración 38800: x = -33.0, grad = 1000.0\n",
            "Iteración 38900: x = -33.0, grad = 1000.0\n",
            "Iteración 39000: x = -33.0, grad = 1000.0\n",
            "Iteración 39100: x = -33.0, grad = 1000.0\n",
            "Iteración 39200: x = -33.0, grad = 1000.0\n",
            "Iteración 39300: x = -33.0, grad = 1000.0\n",
            "Iteración 39400: x = -33.0, grad = 1000.0\n",
            "Iteración 39500: x = -33.0, grad = 1000.0\n",
            "Iteración 39600: x = -33.0, grad = 1000.0\n",
            "Iteración 39700: x = -33.0, grad = 1000.0\n",
            "Iteración 39800: x = -33.0, grad = 1000.0\n",
            "Iteración 39900: x = -33.0, grad = 1000.0\n",
            "Iteración 40000: x = -33.0, grad = 1000.0\n",
            "Iteración 40100: x = -33.0, grad = 1000.0\n",
            "Iteración 40200: x = -33.0, grad = 1000.0\n",
            "Iteración 40300: x = -33.0, grad = 1000.0\n",
            "Iteración 40400: x = -33.0, grad = 1000.0\n",
            "Iteración 40500: x = -33.0, grad = 1000.0\n",
            "Iteración 40600: x = -33.0, grad = 1000.0\n",
            "Iteración 40700: x = -33.0, grad = 1000.0\n",
            "Iteración 40800: x = -33.0, grad = 1000.0\n",
            "Iteración 40900: x = -33.0, grad = 1000.0\n",
            "Iteración 41000: x = -33.0, grad = 1000.0\n",
            "Iteración 41100: x = -33.0, grad = 1000.0\n",
            "Iteración 41200: x = -33.0, grad = 1000.0\n",
            "Iteración 41300: x = -33.0, grad = 1000.0\n",
            "Iteración 41400: x = -33.0, grad = 1000.0\n",
            "Iteración 41500: x = -33.0, grad = 1000.0\n",
            "Iteración 41600: x = -33.0, grad = 1000.0\n",
            "Iteración 41700: x = -33.0, grad = 1000.0\n",
            "Iteración 41800: x = -33.0, grad = 1000.0\n",
            "Iteración 41900: x = -33.0, grad = 1000.0\n",
            "Iteración 42000: x = -33.0, grad = 1000.0\n",
            "Iteración 42100: x = -33.0, grad = 1000.0\n",
            "Iteración 42200: x = -33.0, grad = 1000.0\n",
            "Iteración 42300: x = -33.0, grad = 1000.0\n",
            "Iteración 42400: x = -33.0, grad = 1000.0\n",
            "Iteración 42500: x = -33.0, grad = 1000.0\n",
            "Iteración 42600: x = -33.0, grad = 1000.0\n",
            "Iteración 42700: x = -33.0, grad = 1000.0\n",
            "Iteración 42800: x = -33.0, grad = 1000.0\n",
            "Iteración 42900: x = -33.0, grad = 1000.0\n",
            "Iteración 43000: x = -33.0, grad = 1000.0\n",
            "Iteración 43100: x = -33.0, grad = 1000.0\n",
            "Iteración 43200: x = -33.0, grad = 1000.0\n",
            "Iteración 43300: x = -33.0, grad = 1000.0\n",
            "Iteración 43400: x = -33.0, grad = 1000.0\n",
            "Iteración 43500: x = -33.0, grad = 1000.0\n",
            "Iteración 43600: x = -33.0, grad = 1000.0\n",
            "Iteración 43700: x = -33.0, grad = 1000.0\n",
            "Iteración 43800: x = -33.0, grad = 1000.0\n",
            "Iteración 43900: x = -33.0, grad = 1000.0\n",
            "Iteración 44000: x = -33.0, grad = 1000.0\n",
            "Iteración 44100: x = -33.0, grad = 1000.0\n",
            "Iteración 44200: x = -33.0, grad = 1000.0\n",
            "Iteración 44300: x = -33.0, grad = 1000.0\n",
            "Iteración 44400: x = -33.0, grad = 1000.0\n",
            "Iteración 44500: x = -33.0, grad = 1000.0\n",
            "Iteración 44600: x = -33.0, grad = 1000.0\n",
            "Iteración 44700: x = -33.0, grad = 1000.0\n",
            "Iteración 44800: x = -33.0, grad = 1000.0\n",
            "Iteración 44900: x = -33.0, grad = 1000.0\n",
            "Iteración 45000: x = -33.0, grad = 1000.0\n",
            "Iteración 45100: x = -33.0, grad = 1000.0\n",
            "Iteración 45200: x = -33.0, grad = 1000.0\n",
            "Iteración 45300: x = -33.0, grad = 1000.0\n",
            "Iteración 45400: x = -33.0, grad = 1000.0\n",
            "Iteración 45500: x = -33.0, grad = 1000.0\n",
            "Iteración 45600: x = -33.0, grad = 1000.0\n",
            "Iteración 45700: x = -33.0, grad = 1000.0\n",
            "Iteración 45800: x = -33.0, grad = 1000.0\n",
            "Iteración 45900: x = -33.0, grad = 1000.0\n",
            "Iteración 46000: x = -33.0, grad = 1000.0\n",
            "Iteración 46100: x = -33.0, grad = 1000.0\n",
            "Iteración 46200: x = -33.0, grad = 1000.0\n",
            "Iteración 46300: x = -33.0, grad = 1000.0\n",
            "Iteración 46400: x = -33.0, grad = 1000.0\n",
            "Iteración 46500: x = -33.0, grad = 1000.0\n",
            "Iteración 46600: x = -33.0, grad = 1000.0\n",
            "Iteración 46700: x = -33.0, grad = 1000.0\n",
            "Iteración 46800: x = -33.0, grad = 1000.0\n",
            "Iteración 46900: x = -33.0, grad = 1000.0\n",
            "Iteración 47000: x = -33.0, grad = 1000.0\n",
            "Iteración 47100: x = -33.0, grad = 1000.0\n",
            "Iteración 47200: x = -33.0, grad = 1000.0\n",
            "Iteración 47300: x = -33.0, grad = 1000.0\n",
            "Iteración 47400: x = -33.0, grad = 1000.0\n",
            "Iteración 47500: x = -33.0, grad = 1000.0\n",
            "Iteración 47600: x = -33.0, grad = 1000.0\n",
            "Iteración 47700: x = -33.0, grad = 1000.0\n",
            "Iteración 47800: x = -33.0, grad = 1000.0\n",
            "Iteración 47900: x = -33.0, grad = 1000.0\n",
            "Iteración 48000: x = -33.0, grad = 1000.0\n",
            "Iteración 48100: x = -33.0, grad = 1000.0\n",
            "Iteración 48200: x = -33.0, grad = 1000.0\n",
            "Iteración 48300: x = -33.0, grad = 1000.0\n",
            "Iteración 48400: x = -33.0, grad = 1000.0\n",
            "Iteración 48500: x = -33.0, grad = 1000.0\n",
            "Iteración 48600: x = -33.0, grad = 1000.0\n",
            "Iteración 48700: x = -33.0, grad = 1000.0\n",
            "Iteración 48800: x = -33.0, grad = 1000.0\n",
            "Iteración 48900: x = -33.0, grad = 1000.0\n",
            "Iteración 49000: x = -33.0, grad = 1000.0\n",
            "Iteración 49100: x = -33.0, grad = 1000.0\n",
            "Iteración 49200: x = -33.0, grad = 1000.0\n",
            "Iteración 49300: x = -33.0, grad = 1000.0\n",
            "Iteración 49400: x = -33.0, grad = 1000.0\n",
            "Iteración 49500: x = -33.0, grad = 1000.0\n",
            "Iteración 49600: x = -33.0, grad = 1000.0\n",
            "Iteración 49700: x = -33.0, grad = 1000.0\n",
            "Iteración 49800: x = -33.0, grad = 1000.0\n",
            "Iteración 49900: x = -33.0, grad = 1000.0\n",
            "Iteración 50000: x = -33.0, grad = 1000.0\n",
            "Iteración 50100: x = -33.0, grad = 1000.0\n",
            "Iteración 50200: x = -33.0, grad = 1000.0\n",
            "Iteración 50300: x = -33.0, grad = 1000.0\n",
            "Iteración 50400: x = -33.0, grad = 1000.0\n",
            "Iteración 50500: x = -33.0, grad = 1000.0\n",
            "Iteración 50600: x = -33.0, grad = 1000.0\n",
            "Iteración 50700: x = -33.0, grad = 1000.0\n",
            "Iteración 50800: x = -33.0, grad = 1000.0\n",
            "Iteración 50900: x = -33.0, grad = 1000.0\n",
            "Iteración 51000: x = -33.0, grad = 1000.0\n",
            "Iteración 51100: x = -33.0, grad = 1000.0\n",
            "Iteración 51200: x = -33.0, grad = 1000.0\n",
            "Iteración 51300: x = -33.0, grad = 1000.0\n",
            "Iteración 51400: x = -33.0, grad = 1000.0\n",
            "Iteración 51500: x = -33.0, grad = 1000.0\n",
            "Iteración 51600: x = -33.0, grad = 1000.0\n",
            "Iteración 51700: x = -33.0, grad = 1000.0\n",
            "Iteración 51800: x = -33.0, grad = 1000.0\n",
            "Iteración 51900: x = -33.0, grad = 1000.0\n",
            "Iteración 52000: x = -33.0, grad = 1000.0\n",
            "Iteración 52100: x = -33.0, grad = 1000.0\n",
            "Iteración 52200: x = -33.0, grad = 1000.0\n",
            "Iteración 52300: x = -33.0, grad = 1000.0\n",
            "Iteración 52400: x = -33.0, grad = 1000.0\n",
            "Iteración 52500: x = -33.0, grad = 1000.0\n",
            "Iteración 52600: x = -33.0, grad = 1000.0\n",
            "Iteración 52700: x = -33.0, grad = 1000.0\n",
            "Iteración 52800: x = -33.0, grad = 1000.0\n",
            "Iteración 52900: x = -33.0, grad = 1000.0\n",
            "Iteración 53000: x = -33.0, grad = 1000.0\n",
            "Iteración 53100: x = -33.0, grad = 1000.0\n",
            "Iteración 53200: x = -33.0, grad = 1000.0\n",
            "Iteración 53300: x = -33.0, grad = 1000.0\n",
            "Iteración 53400: x = -33.0, grad = 1000.0\n",
            "Iteración 53500: x = -33.0, grad = 1000.0\n",
            "Iteración 53600: x = -33.0, grad = 1000.0\n",
            "Iteración 53700: x = -33.0, grad = 1000.0\n",
            "Iteración 53800: x = -33.0, grad = 1000.0\n",
            "Iteración 53900: x = -33.0, grad = 1000.0\n",
            "Iteración 54000: x = -33.0, grad = 1000.0\n",
            "Iteración 54100: x = -33.0, grad = 1000.0\n",
            "Iteración 54200: x = -33.0, grad = 1000.0\n",
            "Iteración 54300: x = -33.0, grad = 1000.0\n",
            "Iteración 54400: x = -33.0, grad = 1000.0\n",
            "Iteración 54500: x = -33.0, grad = 1000.0\n",
            "Iteración 54600: x = -33.0, grad = 1000.0\n",
            "Iteración 54700: x = -33.0, grad = 1000.0\n",
            "Iteración 54800: x = -33.0, grad = 1000.0\n",
            "Iteración 54900: x = -33.0, grad = 1000.0\n",
            "Iteración 55000: x = -33.0, grad = 1000.0\n",
            "Iteración 55100: x = -33.0, grad = 1000.0\n",
            "Iteración 55200: x = -33.0, grad = 1000.0\n",
            "Iteración 55300: x = -33.0, grad = 1000.0\n",
            "Iteración 55400: x = -33.0, grad = 1000.0\n",
            "Iteración 55500: x = -33.0, grad = 1000.0\n",
            "Iteración 55600: x = -33.0, grad = 1000.0\n",
            "Iteración 55700: x = -33.0, grad = 1000.0\n",
            "Iteración 55800: x = -33.0, grad = 1000.0\n",
            "Iteración 55900: x = -33.0, grad = 1000.0\n",
            "Iteración 56000: x = -33.0, grad = 1000.0\n",
            "Iteración 56100: x = -33.0, grad = 1000.0\n",
            "Iteración 56200: x = -33.0, grad = 1000.0\n",
            "Iteración 56300: x = -33.0, grad = 1000.0\n",
            "Iteración 56400: x = -33.0, grad = 1000.0\n",
            "Iteración 56500: x = -33.0, grad = 1000.0\n",
            "Iteración 56600: x = -33.0, grad = 1000.0\n",
            "Iteración 56700: x = -33.0, grad = 1000.0\n",
            "Iteración 56800: x = -33.0, grad = 1000.0\n",
            "Iteración 56900: x = -33.0, grad = 1000.0\n",
            "Iteración 57000: x = -33.0, grad = 1000.0\n",
            "Iteración 57100: x = -33.0, grad = 1000.0\n",
            "Iteración 57200: x = -33.0, grad = 1000.0\n",
            "Iteración 57300: x = -33.0, grad = 1000.0\n",
            "Iteración 57400: x = -33.0, grad = 1000.0\n",
            "Iteración 57500: x = -33.0, grad = 1000.0\n",
            "Iteración 57600: x = -33.0, grad = 1000.0\n",
            "Iteración 57700: x = -33.0, grad = 1000.0\n",
            "Iteración 57800: x = -33.0, grad = 1000.0\n",
            "Iteración 57900: x = -33.0, grad = 1000.0\n",
            "Iteración 58000: x = -33.0, grad = 1000.0\n",
            "Iteración 58100: x = -33.0, grad = 1000.0\n",
            "Iteración 58200: x = -33.0, grad = 1000.0\n",
            "Iteración 58300: x = -33.0, grad = 1000.0\n",
            "Iteración 58400: x = -33.0, grad = 1000.0\n",
            "Iteración 58500: x = -33.0, grad = 1000.0\n",
            "Iteración 58600: x = -33.0, grad = 1000.0\n",
            "Iteración 58700: x = -33.0, grad = 1000.0\n",
            "Iteración 58800: x = -33.0, grad = 1000.0\n",
            "Iteración 58900: x = -33.0, grad = 1000.0\n",
            "Iteración 59000: x = -33.0, grad = 1000.0\n",
            "Iteración 59100: x = -33.0, grad = 1000.0\n",
            "Iteración 59200: x = -33.0, grad = 1000.0\n",
            "Iteración 59300: x = -33.0, grad = 1000.0\n",
            "Iteración 59400: x = -33.0, grad = 1000.0\n",
            "Iteración 59500: x = -33.0, grad = 1000.0\n",
            "Iteración 59600: x = -33.0, grad = 1000.0\n",
            "Iteración 59700: x = -33.0, grad = 1000.0\n",
            "Iteración 59800: x = -33.0, grad = 1000.0\n",
            "Iteración 59900: x = -33.0, grad = 1000.0\n",
            "Iteración 60000: x = -33.0, grad = 1000.0\n",
            "Iteración 60100: x = -33.0, grad = 1000.0\n",
            "Iteración 60200: x = -33.0, grad = 1000.0\n",
            "Iteración 60300: x = -33.0, grad = 1000.0\n",
            "Iteración 60400: x = -33.0, grad = 1000.0\n",
            "Iteración 60500: x = -33.0, grad = 1000.0\n",
            "Iteración 60600: x = -33.0, grad = 1000.0\n",
            "Iteración 60700: x = -33.0, grad = 1000.0\n",
            "Iteración 60800: x = -33.0, grad = 1000.0\n",
            "Iteración 60900: x = -33.0, grad = 1000.0\n",
            "Iteración 61000: x = -33.0, grad = 1000.0\n",
            "Iteración 61100: x = -33.0, grad = 1000.0\n",
            "Iteración 61200: x = -33.0, grad = 1000.0\n",
            "Iteración 61300: x = -33.0, grad = 1000.0\n",
            "Iteración 61400: x = -33.0, grad = 1000.0\n",
            "Iteración 61500: x = -33.0, grad = 1000.0\n",
            "Iteración 61600: x = -33.0, grad = 1000.0\n",
            "Iteración 61700: x = -33.0, grad = 1000.0\n",
            "Iteración 61800: x = -33.0, grad = 1000.0\n",
            "Iteración 61900: x = -33.0, grad = 1000.0\n",
            "Iteración 62000: x = -33.0, grad = 1000.0\n",
            "Iteración 62100: x = -33.0, grad = 1000.0\n",
            "Iteración 62200: x = -33.0, grad = 1000.0\n",
            "Iteración 62300: x = -33.0, grad = 1000.0\n",
            "Iteración 62400: x = -33.0, grad = 1000.0\n",
            "Iteración 62500: x = -33.0, grad = 1000.0\n",
            "Iteración 62600: x = -33.0, grad = 1000.0\n",
            "Iteración 62700: x = -33.0, grad = 1000.0\n",
            "Iteración 62800: x = -33.0, grad = 1000.0\n",
            "Iteración 62900: x = -33.0, grad = 1000.0\n",
            "Iteración 63000: x = -33.0, grad = 1000.0\n",
            "Iteración 63100: x = -33.0, grad = 1000.0\n",
            "Iteración 63200: x = -33.0, grad = 1000.0\n",
            "Iteración 63300: x = -33.0, grad = 1000.0\n",
            "Iteración 63400: x = -33.0, grad = 1000.0\n",
            "Iteración 63500: x = -33.0, grad = 1000.0\n",
            "Iteración 63600: x = -33.0, grad = 1000.0\n",
            "Iteración 63700: x = -33.0, grad = 1000.0\n",
            "Iteración 63800: x = -33.0, grad = 1000.0\n",
            "Iteración 63900: x = -33.0, grad = 1000.0\n",
            "Iteración 64000: x = -33.0, grad = 1000.0\n",
            "Iteración 64100: x = -33.0, grad = 1000.0\n",
            "Iteración 64200: x = -33.0, grad = 1000.0\n",
            "Iteración 64300: x = -33.0, grad = 1000.0\n",
            "Iteración 64400: x = -33.0, grad = 1000.0\n",
            "Iteración 64500: x = -33.0, grad = 1000.0\n",
            "Iteración 64600: x = -33.0, grad = 1000.0\n",
            "Iteración 64700: x = -33.0, grad = 1000.0\n",
            "Iteración 64800: x = -33.0, grad = 1000.0\n",
            "Iteración 64900: x = -33.0, grad = 1000.0\n",
            "Iteración 65000: x = -33.0, grad = 1000.0\n",
            "Iteración 65100: x = -33.0, grad = 1000.0\n",
            "Iteración 65200: x = -33.0, grad = 1000.0\n",
            "Iteración 65300: x = -33.0, grad = 1000.0\n",
            "Iteración 65400: x = -33.0, grad = 1000.0\n",
            "Iteración 65500: x = -33.0, grad = 1000.0\n",
            "Iteración 65600: x = -33.0, grad = 1000.0\n",
            "Iteración 65700: x = -33.0, grad = 1000.0\n",
            "Iteración 65800: x = -33.0, grad = 1000.0\n",
            "Iteración 65900: x = -33.0, grad = 1000.0\n",
            "Iteración 66000: x = -33.0, grad = 1000.0\n",
            "Iteración 66100: x = -33.0, grad = 1000.0\n",
            "Iteración 66200: x = -33.0, grad = 1000.0\n",
            "Iteración 66300: x = -33.0, grad = 1000.0\n",
            "Iteración 66400: x = -33.0, grad = 1000.0\n",
            "Iteración 66500: x = -33.0, grad = 1000.0\n",
            "Iteración 66600: x = -33.0, grad = 1000.0\n",
            "Iteración 66700: x = -33.0, grad = 1000.0\n",
            "Iteración 66800: x = -33.0, grad = 1000.0\n",
            "Iteración 66900: x = -33.0, grad = 1000.0\n",
            "Iteración 67000: x = -33.0, grad = 1000.0\n",
            "Iteración 67100: x = -33.0, grad = 1000.0\n",
            "Iteración 67200: x = -33.0, grad = 1000.0\n",
            "Iteración 67300: x = -33.0, grad = 1000.0\n",
            "Iteración 67400: x = -33.0, grad = 1000.0\n",
            "Iteración 67500: x = -33.0, grad = 1000.0\n",
            "Iteración 67600: x = -33.0, grad = 1000.0\n",
            "Iteración 67700: x = -33.0, grad = 1000.0\n",
            "Iteración 67800: x = -33.0, grad = 1000.0\n",
            "Iteración 67900: x = -33.0, grad = 1000.0\n",
            "Iteración 68000: x = -33.0, grad = 1000.0\n",
            "Iteración 68100: x = -33.0, grad = 1000.0\n",
            "Iteración 68200: x = -33.0, grad = 1000.0\n",
            "Iteración 68300: x = -33.0, grad = 1000.0\n",
            "Iteración 68400: x = -33.0, grad = 1000.0\n",
            "Iteración 68500: x = -33.0, grad = 1000.0\n",
            "Iteración 68600: x = -33.0, grad = 1000.0\n",
            "Iteración 68700: x = -33.0, grad = 1000.0\n",
            "Iteración 68800: x = -33.0, grad = 1000.0\n",
            "Iteración 68900: x = -33.0, grad = 1000.0\n",
            "Iteración 69000: x = -33.0, grad = 1000.0\n",
            "Iteración 69100: x = -33.0, grad = 1000.0\n",
            "Iteración 69200: x = -33.0, grad = 1000.0\n",
            "Iteración 69300: x = -33.0, grad = 1000.0\n",
            "Iteración 69400: x = -33.0, grad = 1000.0\n",
            "Iteración 69500: x = -33.0, grad = 1000.0\n",
            "Iteración 69600: x = -33.0, grad = 1000.0\n",
            "Iteración 69700: x = -33.0, grad = 1000.0\n",
            "Iteración 69800: x = -33.0, grad = 1000.0\n",
            "Iteración 69900: x = -33.0, grad = 1000.0\n",
            "Iteración 70000: x = -33.0, grad = 1000.0\n",
            "Iteración 70100: x = -33.0, grad = 1000.0\n",
            "Iteración 70200: x = -33.0, grad = 1000.0\n",
            "Iteración 70300: x = -33.0, grad = 1000.0\n",
            "Iteración 70400: x = -33.0, grad = 1000.0\n",
            "Iteración 70500: x = -33.0, grad = 1000.0\n",
            "Iteración 70600: x = -33.0, grad = 1000.0\n",
            "Iteración 70700: x = -33.0, grad = 1000.0\n",
            "Iteración 70800: x = -33.0, grad = 1000.0\n",
            "Iteración 70900: x = -33.0, grad = 1000.0\n",
            "Iteración 71000: x = -33.0, grad = 1000.0\n",
            "Iteración 71100: x = -33.0, grad = 1000.0\n",
            "Iteración 71200: x = -33.0, grad = 1000.0\n",
            "Iteración 71300: x = -33.0, grad = 1000.0\n",
            "Iteración 71400: x = -33.0, grad = 1000.0\n",
            "Iteración 71500: x = -33.0, grad = 1000.0\n",
            "Iteración 71600: x = -33.0, grad = 1000.0\n",
            "Iteración 71700: x = -33.0, grad = 1000.0\n",
            "Iteración 71800: x = -33.0, grad = 1000.0\n",
            "Iteración 71900: x = -33.0, grad = 1000.0\n",
            "Iteración 72000: x = -33.0, grad = 1000.0\n",
            "Iteración 72100: x = -33.0, grad = 1000.0\n",
            "Iteración 72200: x = -33.0, grad = 1000.0\n",
            "Iteración 72300: x = -33.0, grad = 1000.0\n",
            "Iteración 72400: x = -33.0, grad = 1000.0\n",
            "Iteración 72500: x = -33.0, grad = 1000.0\n",
            "Iteración 72600: x = -33.0, grad = 1000.0\n",
            "Iteración 72700: x = -33.0, grad = 1000.0\n",
            "Iteración 72800: x = -33.0, grad = 1000.0\n",
            "Iteración 72900: x = -33.0, grad = 1000.0\n",
            "Iteración 73000: x = -33.0, grad = 1000.0\n",
            "Iteración 73100: x = -33.0, grad = 1000.0\n",
            "Iteración 73200: x = -33.0, grad = 1000.0\n",
            "Iteración 73300: x = -33.0, grad = 1000.0\n",
            "Iteración 73400: x = -33.0, grad = 1000.0\n",
            "Iteración 73500: x = -33.0, grad = 1000.0\n",
            "Iteración 73600: x = -33.0, grad = 1000.0\n",
            "Iteración 73700: x = -33.0, grad = 1000.0\n",
            "Iteración 73800: x = -33.0, grad = 1000.0\n",
            "Iteración 73900: x = -33.0, grad = 1000.0\n",
            "Iteración 74000: x = -33.0, grad = 1000.0\n",
            "Iteración 74100: x = -33.0, grad = 1000.0\n",
            "Iteración 74200: x = -33.0, grad = 1000.0\n",
            "Iteración 74300: x = -33.0, grad = 1000.0\n",
            "Iteración 74400: x = -33.0, grad = 1000.0\n",
            "Iteración 74500: x = -33.0, grad = 1000.0\n",
            "Iteración 74600: x = -33.0, grad = 1000.0\n",
            "Iteración 74700: x = -33.0, grad = 1000.0\n",
            "Iteración 74800: x = -33.0, grad = 1000.0\n",
            "Iteración 74900: x = -33.0, grad = 1000.0\n",
            "Iteración 75000: x = -33.0, grad = 1000.0\n",
            "Iteración 75100: x = -33.0, grad = 1000.0\n",
            "Iteración 75200: x = -33.0, grad = 1000.0\n",
            "Iteración 75300: x = -33.0, grad = 1000.0\n",
            "Iteración 75400: x = -33.0, grad = 1000.0\n",
            "Iteración 75500: x = -33.0, grad = 1000.0\n",
            "Iteración 75600: x = -33.0, grad = 1000.0\n",
            "Iteración 75700: x = -33.0, grad = 1000.0\n",
            "Iteración 75800: x = -33.0, grad = 1000.0\n",
            "Iteración 75900: x = -33.0, grad = 1000.0\n",
            "Iteración 76000: x = -33.0, grad = 1000.0\n",
            "Iteración 76100: x = -33.0, grad = 1000.0\n",
            "Iteración 76200: x = -33.0, grad = 1000.0\n",
            "Iteración 76300: x = -33.0, grad = 1000.0\n",
            "Iteración 76400: x = -33.0, grad = 1000.0\n",
            "Iteración 76500: x = -33.0, grad = 1000.0\n",
            "Iteración 76600: x = -33.0, grad = 1000.0\n",
            "Iteración 76700: x = -33.0, grad = 1000.0\n",
            "Iteración 76800: x = -33.0, grad = 1000.0\n",
            "Iteración 76900: x = -33.0, grad = 1000.0\n",
            "Iteración 77000: x = -33.0, grad = 1000.0\n",
            "Iteración 77100: x = -33.0, grad = 1000.0\n",
            "Iteración 77200: x = -33.0, grad = 1000.0\n",
            "Iteración 77300: x = -33.0, grad = 1000.0\n",
            "Iteración 77400: x = -33.0, grad = 1000.0\n",
            "Iteración 77500: x = -33.0, grad = 1000.0\n",
            "Iteración 77600: x = -33.0, grad = 1000.0\n",
            "Iteración 77700: x = -33.0, grad = 1000.0\n",
            "Iteración 77800: x = -33.0, grad = 1000.0\n",
            "Iteración 77900: x = -33.0, grad = 1000.0\n",
            "Iteración 78000: x = -33.0, grad = 1000.0\n",
            "Iteración 78100: x = -33.0, grad = 1000.0\n",
            "Iteración 78200: x = -33.0, grad = 1000.0\n",
            "Iteración 78300: x = -33.0, grad = 1000.0\n",
            "Iteración 78400: x = -33.0, grad = 1000.0\n",
            "Iteración 78500: x = -33.0, grad = 1000.0\n",
            "Iteración 78600: x = -33.0, grad = 1000.0\n",
            "Iteración 78700: x = -33.0, grad = 1000.0\n",
            "Iteración 78800: x = -33.0, grad = 1000.0\n",
            "Iteración 78900: x = -33.0, grad = 1000.0\n",
            "Iteración 79000: x = -33.0, grad = 1000.0\n",
            "Iteración 79100: x = -33.0, grad = 1000.0\n",
            "Iteración 79200: x = -33.0, grad = 1000.0\n",
            "Iteración 79300: x = -33.0, grad = 1000.0\n",
            "Iteración 79400: x = -33.0, grad = 1000.0\n",
            "Iteración 79500: x = -33.0, grad = 1000.0\n",
            "Iteración 79600: x = -33.0, grad = 1000.0\n",
            "Iteración 79700: x = -33.0, grad = 1000.0\n",
            "Iteración 79800: x = -33.0, grad = 1000.0\n",
            "Iteración 79900: x = -33.0, grad = 1000.0\n",
            "Iteración 80000: x = -33.0, grad = 1000.0\n",
            "Iteración 80100: x = -33.0, grad = 1000.0\n",
            "Iteración 80200: x = -33.0, grad = 1000.0\n",
            "Iteración 80300: x = -33.0, grad = 1000.0\n",
            "Iteración 80400: x = -33.0, grad = 1000.0\n",
            "Iteración 80500: x = -33.0, grad = 1000.0\n",
            "Iteración 80600: x = -33.0, grad = 1000.0\n",
            "Iteración 80700: x = -33.0, grad = 1000.0\n",
            "Iteración 80800: x = -33.0, grad = 1000.0\n",
            "Iteración 80900: x = -33.0, grad = 1000.0\n",
            "Iteración 81000: x = -33.0, grad = 1000.0\n",
            "Iteración 81100: x = -33.0, grad = 1000.0\n",
            "Iteración 81200: x = -33.0, grad = 1000.0\n",
            "Iteración 81300: x = -33.0, grad = 1000.0\n",
            "Iteración 81400: x = -33.0, grad = 1000.0\n",
            "Iteración 81500: x = -33.0, grad = 1000.0\n",
            "Iteración 81600: x = -33.0, grad = 1000.0\n",
            "Iteración 81700: x = -33.0, grad = 1000.0\n",
            "Iteración 81800: x = -33.0, grad = 1000.0\n",
            "Iteración 81900: x = -33.0, grad = 1000.0\n",
            "Iteración 82000: x = -33.0, grad = 1000.0\n",
            "Iteración 82100: x = -33.0, grad = 1000.0\n",
            "Iteración 82200: x = -33.0, grad = 1000.0\n",
            "Iteración 82300: x = -33.0, grad = 1000.0\n",
            "Iteración 82400: x = -33.0, grad = 1000.0\n",
            "Iteración 82500: x = -33.0, grad = 1000.0\n",
            "Iteración 82600: x = -33.0, grad = 1000.0\n",
            "Iteración 82700: x = -33.0, grad = 1000.0\n",
            "Iteración 82800: x = -33.0, grad = 1000.0\n",
            "Iteración 82900: x = -33.0, grad = 1000.0\n",
            "Iteración 83000: x = -33.0, grad = 1000.0\n",
            "Iteración 83100: x = -33.0, grad = 1000.0\n",
            "Iteración 83200: x = -33.0, grad = 1000.0\n",
            "Iteración 83300: x = -33.0, grad = 1000.0\n",
            "Iteración 83400: x = -33.0, grad = 1000.0\n",
            "Iteración 83500: x = -33.0, grad = 1000.0\n",
            "Iteración 83600: x = -33.0, grad = 1000.0\n",
            "Iteración 83700: x = -33.0, grad = 1000.0\n",
            "Iteración 83800: x = -33.0, grad = 1000.0\n",
            "Iteración 83900: x = -33.0, grad = 1000.0\n",
            "Iteración 84000: x = -33.0, grad = 1000.0\n",
            "Iteración 84100: x = -33.0, grad = 1000.0\n",
            "Iteración 84200: x = -33.0, grad = 1000.0\n",
            "Iteración 84300: x = -33.0, grad = 1000.0\n",
            "Iteración 84400: x = -33.0, grad = 1000.0\n",
            "Iteración 84500: x = -33.0, grad = 1000.0\n",
            "Iteración 84600: x = -33.0, grad = 1000.0\n",
            "Iteración 84700: x = -33.0, grad = 1000.0\n",
            "Iteración 84800: x = -33.0, grad = 1000.0\n",
            "Iteración 84900: x = -33.0, grad = 1000.0\n",
            "Iteración 85000: x = -33.0, grad = 1000.0\n",
            "Iteración 85100: x = -33.0, grad = 1000.0\n",
            "Iteración 85200: x = -33.0, grad = 1000.0\n",
            "Iteración 85300: x = -33.0, grad = 1000.0\n",
            "Iteración 85400: x = -33.0, grad = 1000.0\n",
            "Iteración 85500: x = -33.0, grad = 1000.0\n",
            "Iteración 85600: x = -33.0, grad = 1000.0\n",
            "Iteración 85700: x = -33.0, grad = 1000.0\n",
            "Iteración 85800: x = -33.0, grad = 1000.0\n",
            "Iteración 85900: x = -33.0, grad = 1000.0\n",
            "Iteración 86000: x = -33.0, grad = 1000.0\n",
            "Iteración 86100: x = -33.0, grad = 1000.0\n",
            "Iteración 86200: x = -33.0, grad = 1000.0\n",
            "Iteración 86300: x = -33.0, grad = 1000.0\n",
            "Iteración 86400: x = -33.0, grad = 1000.0\n",
            "Iteración 86500: x = -33.0, grad = 1000.0\n",
            "Iteración 86600: x = -33.0, grad = 1000.0\n",
            "Iteración 86700: x = -33.0, grad = 1000.0\n",
            "Iteración 86800: x = -33.0, grad = 1000.0\n",
            "Iteración 86900: x = -33.0, grad = 1000.0\n",
            "Iteración 87000: x = -33.0, grad = 1000.0\n",
            "Iteración 87100: x = -33.0, grad = 1000.0\n",
            "Iteración 87200: x = -33.0, grad = 1000.0\n",
            "Iteración 87300: x = -33.0, grad = 1000.0\n",
            "Iteración 87400: x = -33.0, grad = 1000.0\n",
            "Iteración 87500: x = -33.0, grad = 1000.0\n",
            "Iteración 87600: x = -33.0, grad = 1000.0\n",
            "Iteración 87700: x = -33.0, grad = 1000.0\n",
            "Iteración 87800: x = -33.0, grad = 1000.0\n",
            "Iteración 87900: x = -33.0, grad = 1000.0\n",
            "Iteración 88000: x = -33.0, grad = 1000.0\n",
            "Iteración 88100: x = -33.0, grad = 1000.0\n",
            "Iteración 88200: x = -33.0, grad = 1000.0\n",
            "Iteración 88300: x = -33.0, grad = 1000.0\n",
            "Iteración 88400: x = -33.0, grad = 1000.0\n",
            "Iteración 88500: x = -33.0, grad = 1000.0\n",
            "Iteración 88600: x = -33.0, grad = 1000.0\n",
            "Iteración 88700: x = -33.0, grad = 1000.0\n",
            "Iteración 88800: x = -33.0, grad = 1000.0\n",
            "Iteración 88900: x = -33.0, grad = 1000.0\n",
            "Iteración 89000: x = -33.0, grad = 1000.0\n",
            "Iteración 89100: x = -33.0, grad = 1000.0\n",
            "Iteración 89200: x = -33.0, grad = 1000.0\n",
            "Iteración 89300: x = -33.0, grad = 1000.0\n",
            "Iteración 89400: x = -33.0, grad = 1000.0\n",
            "Iteración 89500: x = -33.0, grad = 1000.0\n",
            "Iteración 89600: x = -33.0, grad = 1000.0\n",
            "Iteración 89700: x = -33.0, grad = 1000.0\n",
            "Iteración 89800: x = -33.0, grad = 1000.0\n",
            "Iteración 89900: x = -33.0, grad = 1000.0\n",
            "Iteración 90000: x = -33.0, grad = 1000.0\n",
            "Iteración 90100: x = -33.0, grad = 1000.0\n",
            "Iteración 90200: x = -33.0, grad = 1000.0\n",
            "Iteración 90300: x = -33.0, grad = 1000.0\n",
            "Iteración 90400: x = -33.0, grad = 1000.0\n",
            "Iteración 90500: x = -33.0, grad = 1000.0\n",
            "Iteración 90600: x = -33.0, grad = 1000.0\n",
            "Iteración 90700: x = -33.0, grad = 1000.0\n",
            "Iteración 90800: x = -33.0, grad = 1000.0\n",
            "Iteración 90900: x = -33.0, grad = 1000.0\n",
            "Iteración 91000: x = -33.0, grad = 1000.0\n",
            "Iteración 91100: x = -33.0, grad = 1000.0\n",
            "Iteración 91200: x = -33.0, grad = 1000.0\n",
            "Iteración 91300: x = -33.0, grad = 1000.0\n",
            "Iteración 91400: x = -33.0, grad = 1000.0\n",
            "Iteración 91500: x = -33.0, grad = 1000.0\n",
            "Iteración 91600: x = -33.0, grad = 1000.0\n",
            "Iteración 91700: x = -33.0, grad = 1000.0\n",
            "Iteración 91800: x = -33.0, grad = 1000.0\n",
            "Iteración 91900: x = -33.0, grad = 1000.0\n",
            "Iteración 92000: x = -33.0, grad = 1000.0\n",
            "Iteración 92100: x = -33.0, grad = 1000.0\n",
            "Iteración 92200: x = -33.0, grad = 1000.0\n",
            "Iteración 92300: x = -33.0, grad = 1000.0\n",
            "Iteración 92400: x = -33.0, grad = 1000.0\n",
            "Iteración 92500: x = -33.0, grad = 1000.0\n",
            "Iteración 92600: x = -33.0, grad = 1000.0\n",
            "Iteración 92700: x = -33.0, grad = 1000.0\n",
            "Iteración 92800: x = -33.0, grad = 1000.0\n",
            "Iteración 92900: x = -33.0, grad = 1000.0\n",
            "Iteración 93000: x = -33.0, grad = 1000.0\n",
            "Iteración 93100: x = -33.0, grad = 1000.0\n",
            "Iteración 93200: x = -33.0, grad = 1000.0\n",
            "Iteración 93300: x = -33.0, grad = 1000.0\n",
            "Iteración 93400: x = -33.0, grad = 1000.0\n",
            "Iteración 93500: x = -33.0, grad = 1000.0\n",
            "Iteración 93600: x = -33.0, grad = 1000.0\n",
            "Iteración 93700: x = -33.0, grad = 1000.0\n",
            "Iteración 93800: x = -33.0, grad = 1000.0\n",
            "Iteración 93900: x = -33.0, grad = 1000.0\n",
            "Iteración 94000: x = -33.0, grad = 1000.0\n",
            "Iteración 94100: x = -33.0, grad = 1000.0\n",
            "Iteración 94200: x = -33.0, grad = 1000.0\n",
            "Iteración 94300: x = -33.0, grad = 1000.0\n",
            "Iteración 94400: x = -33.0, grad = 1000.0\n",
            "Iteración 94500: x = -33.0, grad = 1000.0\n",
            "Iteración 94600: x = -33.0, grad = 1000.0\n",
            "Iteración 94700: x = -33.0, grad = 1000.0\n",
            "Iteración 94800: x = -33.0, grad = 1000.0\n",
            "Iteración 94900: x = -33.0, grad = 1000.0\n",
            "Iteración 95000: x = -33.0, grad = 1000.0\n",
            "Iteración 95100: x = -33.0, grad = 1000.0\n",
            "Iteración 95200: x = -33.0, grad = 1000.0\n",
            "Iteración 95300: x = -33.0, grad = 1000.0\n",
            "Iteración 95400: x = -33.0, grad = 1000.0\n",
            "Iteración 95500: x = -33.0, grad = 1000.0\n",
            "Iteración 95600: x = -33.0, grad = 1000.0\n",
            "Iteración 95700: x = -33.0, grad = 1000.0\n",
            "Iteración 95800: x = -33.0, grad = 1000.0\n",
            "Iteración 95900: x = -33.0, grad = 1000.0\n",
            "Iteración 96000: x = -33.0, grad = 1000.0\n",
            "Iteración 96100: x = -33.0, grad = 1000.0\n",
            "Iteración 96200: x = -33.0, grad = 1000.0\n",
            "Iteración 96300: x = -33.0, grad = 1000.0\n",
            "Iteración 96400: x = -33.0, grad = 1000.0\n",
            "Iteración 96500: x = -33.0, grad = 1000.0\n",
            "Iteración 96600: x = -33.0, grad = 1000.0\n",
            "Iteración 96700: x = -33.0, grad = 1000.0\n",
            "Iteración 96800: x = -33.0, grad = 1000.0\n",
            "Iteración 96900: x = -33.0, grad = 1000.0\n",
            "Iteración 97000: x = -33.0, grad = 1000.0\n",
            "Iteración 97100: x = -33.0, grad = 1000.0\n",
            "Iteración 97200: x = -33.0, grad = 1000.0\n",
            "Iteración 97300: x = -33.0, grad = 1000.0\n",
            "Iteración 97400: x = -33.0, grad = 1000.0\n",
            "Iteración 97500: x = -33.0, grad = 1000.0\n",
            "Iteración 97600: x = -33.0, grad = 1000.0\n",
            "Iteración 97700: x = -33.0, grad = 1000.0\n",
            "Iteración 97800: x = -33.0, grad = 1000.0\n",
            "Iteración 97900: x = -33.0, grad = 1000.0\n",
            "Iteración 98000: x = -33.0, grad = 1000.0\n",
            "Iteración 98100: x = -33.0, grad = 1000.0\n",
            "Iteración 98200: x = -33.0, grad = 1000.0\n",
            "Iteración 98300: x = -33.0, grad = 1000.0\n",
            "Iteración 98400: x = -33.0, grad = 1000.0\n",
            "Iteración 98500: x = -33.0, grad = 1000.0\n",
            "Iteración 98600: x = -33.0, grad = 1000.0\n",
            "Iteración 98700: x = -33.0, grad = 1000.0\n",
            "Iteración 98800: x = -33.0, grad = 1000.0\n",
            "Iteración 98900: x = -33.0, grad = 1000.0\n",
            "Iteración 99000: x = -33.0, grad = 1000.0\n",
            "Iteración 99100: x = -33.0, grad = 1000.0\n",
            "Iteración 99200: x = -33.0, grad = 1000.0\n",
            "Iteración 99300: x = -33.0, grad = 1000.0\n",
            "Iteración 99400: x = -33.0, grad = 1000.0\n",
            "Iteración 99500: x = -33.0, grad = 1000.0\n",
            "Iteración 99600: x = -33.0, grad = 1000.0\n",
            "Iteración 99700: x = -33.0, grad = 1000.0\n",
            "Iteración 99800: x = -33.0, grad = 1000.0\n",
            "Iteración 99900: x = -33.0, grad = 1000.0\n",
            "Iteraciones máximas alcanzadas. Último valor: x = 67.0\n",
            "Resultado con γ = 0.1 (clipping): x ≈ 67.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### v. [0.5 puntos] Finalmente, aplica el método sobre f(x) con $x_0 = 0, γ = 0.001, tol=1e-12, maxit=1e5$. Interpreta el resultado y compáralo con el estudio anal´ıtico de f. ¿Se trata de un resultado deseable? ¿Por qué? ¿A qué se debe este fenómeno?"
      ],
      "metadata": {
        "id": "DFKTEaIJmhjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x0 = 0\n",
        "gamma = 0.001\n",
        "x_sol_4 = gradiente_descendente_1D(grad_f, x0, gamma, tol, maxit)\n",
        "print(f\"Resultado con x0 = 0, γ = 0.001: x ≈ {x_sol_4}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYghVT74skPe",
        "outputId": "f8d24be9-b9f7-40b4-bc3b-17164d1dffa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Convergencia alcanzada en iteración 0, x = 0\n",
            "Resultado con x0 = 0, γ = 0.001: x ≈ 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$x=0$ es un punto de inflexión donde el gradiente es cero, pero no es un mínimo local."
      ],
      "metadata": {
        "id": "G5tmaUrQslrx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## c) Sea la función $g: R^2 →R$ dada por\n",
        "$g(x,y) = x2 + y3 + 3xy+ 1$\n",
        "\n",
        "i [0.5 puntos] Apl´ıquese el método sobre $g(x,y)$ con $x_0 = (−1,1), γ = 0.01,\n",
        "tol=1e-12, maxit=1e5.$\n"
      ],
      "metadata": {
        "id": "rtfkMh9GtR5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def grad_g(xy):\n",
        "    \"\"\"\n",
        "    Calcula el gradiente de la función g(x, y) = x^2 + y^3 + 3xy + 1.\n",
        "    Args:\n",
        "        xy (ndarray): Punto (x, y) donde se evalúa el gradiente.\n",
        "    Returns:\n",
        "        ndarray: Gradiente de g en el punto dado.\n",
        "    \"\"\"\n",
        "    x, y = xy\n",
        "    grad_x = 2 * x + 3 * y       # Derivada parcial respecto a x\n",
        "    grad_y = 3 * y**2 + 3 * x    # Derivada parcial respecto a y\n",
        "    return np.array([grad_x, grad_y])\n",
        "\n",
        "def gradiente_descendente_2D(grad_f, xy0, gamma, tol, maxit):\n",
        "    \"\"\"\n",
        "    Método de descenso de gradiente en 2D.\n",
        "    Args:\n",
        "        grad_f (function): Función que devuelve el gradiente.\n",
        "        xy0 (ndarray): Punto inicial (x0, y0).\n",
        "        gamma (float): Ratio de aprendizaje.\n",
        "        tol (float): Tolerancia para la norma del gradiente.\n",
        "        maxit (int): Máximo número de iteraciones.\n",
        "    Returns:\n",
        "        xy (ndarray): Aproximación al mínimo local.\n",
        "        history (list): Historial de puntos evaluados.\n",
        "    \"\"\"\n",
        "    xy = np.array(xy0, dtype=float)\n",
        "    history = [xy0]\n",
        "\n",
        "    for i in range(int(maxit)):\n",
        "        grad = grad_f(xy)  # Evaluar el gradiente\n",
        "        norm_grad = np.linalg.norm(grad)  # Norma Euclidiana del gradiente\n",
        "\n",
        "        if norm_grad < tol:  # Criterio de convergencia\n",
        "            print(f\"Convergencia alcanzada en iteración {i}, punto: {xy}\")\n",
        "            return xy, history\n",
        "\n",
        "        xy = xy - gamma * grad  # Actualización del punto\n",
        "        history.append(xy)\n",
        "\n",
        "        # Mostrar progreso cada 10000 iteraciones\n",
        "        if i % 10000 == 0:\n",
        "            print(f\"Iteración {i}: Punto = {xy}, ||grad|| = {norm_grad:.6e}\")\n",
        "\n",
        "    print(\"Máximo de iteraciones alcanzado.\")\n",
        "    return xy, history\n",
        "\n",
        "# Parámetros iniciales\n",
        "xy0 = np.array([-1.0, 1.0])  # Punto inicial (x0, y0)\n",
        "gamma = 0.01                 # Ratio de aprendizaje\n",
        "tol = 1e-12                  # Tolerancia\n",
        "maxit = 1e5                  # Máximo de iteraciones\n",
        "\n",
        "# Ejecutar el método\n",
        "resultado, historial = gradiente_descendente_2D(grad_g, xy0, gamma, tol, maxit)\n",
        "\n",
        "# Mostrar el resultado\n",
        "print(f\"\\nResultado final: x ≈ {resultado[0]}, y ≈ {resultado[1]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_0COPLOu26V",
        "outputId": "d369debb-474c-4339-9cb2-d413a799311d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 0: Punto = [-1.01  1.  ], ||grad|| = 1.000000e+00\n",
            "Convergencia alcanzada en iteración 3139, punto: [-2.25  1.5 ]\n",
            "\n",
            "Resultado final: x ≈ -2.2499999999989475, y ≈ 1.4999999999996108\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### ii [0.5 puntos] ¿Qué ocurre si ahora partimos de $x_0 = (0,0)$? ¿Se obtiene un resultado deseable?\n",
        "\n"
      ],
      "metadata": {
        "id": "D0dafSHItza8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parámetros iniciales\n",
        "xy0 = np.array([0, 0])  # Punto inicial (x0, y0)\n",
        "\n",
        "# Ejecutar el método\n",
        "resultado, historial = gradiente_descendente_2D(grad_g, xy0, gamma, tol, maxit)\n",
        "\n",
        "# Mostrar el resultado\n",
        "print(f\"\\nResultado final: x ≈ {resultado[0]}, y ≈ {resultado[1]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dV7rR14xu6iz",
        "outputId": "22c592dc-7017-414f-c2d7-8413f326810b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Convergencia alcanzada en iteración 0, punto: [0. 0.]\n",
            "\n",
            "Resultado final: x ≈ 0.0, y ≈ 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este resultado **no es deseable**, ya que el método se queda atascado en un punto crítico."
      ],
      "metadata": {
        "id": "d67_7LMHxZ66"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### iii [0.5 puntos] Real´ıcese el estudio anal´ıtico de la función y util´ıcese para explicar y contrastar los resultados obtenidos en los dos apartados anteriores."
      ],
      "metadata": {
        "id": "cz2RVwQft0nt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**$( x_0 = (0, 0) $)**\n",
        "\n",
        "Cuando el método de descenso de gradiente parte del punto inicial $ x_0 = (0, 0) $, el algoritmo **concluye inmediatamente** en la **iteración 0**, devolviendo el punto $ (0.0, 0.0) $. Esto ocurre porque el **gradiente** en $ (0, 0) $ es exactamente cero.\n",
        "\n",
        "---\n",
        "\n",
        "### **Cálculo del Gradiente**\n",
        "\n",
        "La función objetivo es:\n",
        "\n",
        "$$\n",
        "g(x, y) = x^2 + y^3 + 3xy + 1\n",
        "$$\n",
        "\n",
        "Su gradiente es:\n",
        "$$\n",
        "\\nabla g(x, y) = \\left( 2x + 3y, \\, 3y^2 + 3x \\right)\n",
        "$$\n",
        "\n",
        "Al evaluar el gradiente en el punto \\( (0, 0) \\), obtenemos:\n",
        "$$\n",
        "\\nabla g(0, 0) = \\left( 2 \\cdot 0 + 3 \\cdot 0, \\, 3 \\cdot 0^2 + 3 \\cdot 0 \\right) = (0, 0)\n",
        "$$\n",
        "\n",
        "Como el gradiente es **cero**, el método interpreta que se ha alcanzado la **convergencia**.\n",
        "\n",
        "---\n",
        "\n",
        "**¿Es $ (0, 0) $ un mínimo local?**\n",
        "\n",
        "Para determinar la naturaleza del punto $ (0, 0) $, calculamos la matriz Hessiana $(H)$:\n",
        "   - $ \\det(H) > 0 $: Mínimo o máximo local.\n",
        "   - $ \\det(H) < 0 $: Punto de silla.\n",
        "\n",
        "$$\n",
        "H =\n",
        "\\begin{bmatrix}\n",
        "\\frac{\\partial^2 g}{\\partial x^2} & \\frac{\\partial^2 g}{\\partial x \\partial y} \\\\\n",
        "\\frac{\\partial^2 g}{\\partial y \\partial x} & \\frac{\\partial^2 g}{\\partial y^2}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Calculamos cada término:\n",
        "\n",
        "- $( \\frac{\\partial^2 g}{\\partial x^2} = 2$)\n",
        "- $( \\frac{\\partial^2 g}{\\partial x \\partial y} = 3 $)\n",
        "- $( \\frac{\\partial^2 g}{\\partial y^2} = 6y \\Rightarrow 0 \\, \\text{en} \\, (0, 0) $)\n",
        "\n",
        "Por lo tanto, la matriz Hessiana en $ (0, 0) $ es:\n",
        "$$\n",
        "H(0, 0) =\n",
        "\\begin{bmatrix}\n",
        "2 & 3 \\\\\n",
        "3 & 0\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "El **determinante** de la Hessiana es:\n",
        "$$\n",
        "\\det(H) = (2)(0) - (3)(3) = -9\n",
        "$$\n",
        "\n",
        "Como el determinante es **negativo**, el punto $ (0, 0) $ es un **punto de silla**, no un mínimo local.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusión**\n",
        "\n",
        "1. El método de descenso de gradiente **se detiene inmediatamente** en $ (0, 0) $ porque el gradiente es cero.\n",
        "2. Sin embargo, $ (0, 0) $ no es un mínimo local, sino un **punto de silla**."
      ],
      "metadata": {
        "id": "U02g__R7vtZ2"
      }
    }
  ]
}